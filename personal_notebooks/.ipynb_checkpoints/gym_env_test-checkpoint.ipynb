{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments, run\n",
    "from ray.tune.experiment import Experiment\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 11:21:54,946\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-10_11-21-54_945912_6445/logs.\n",
      "2020-03-10 11:21:55,059\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:39319 to respond...\n",
      "2020-03-10 11:21:55,205\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:42518 to respond...\n",
      "2020-03-10 11:21:55,217\tINFO services.py:809 -- Starting Redis shard with 1.65 GB max memory.\n",
      "2020-03-10 11:21:55,272\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-10_11-21-54_945912_6445/logs.\n",
      "2020-03-10 11:21:55,275\tWARNING services.py:1330 -- WARNING: The default object store size of 2.47 GB will use more than 50% of the available memory on this node (4.74 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2020-03-10 11:21:55,277\tINFO services.py:1475 -- Starting the Plasma object store with 2.47 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.105',\n",
       " 'redis_address': '192.168.2.105:39319',\n",
       " 'object_store_address': '/tmp/ray/session_2020-03-10_11-21-54_945912_6445/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-03-10_11-21-54_945912_6445/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-03-10_11-21-54_945912_6445'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 4\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(num_cpus=N_CPUS)#, object_store_memory=1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(config):\n",
    "    # ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
    "    # ensure we run at least one sgd iter\n",
    "    if config[\"num_sgd_iter\"] < 1:\n",
    "        config[\"num_sgd_iter\"] = 1\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=4,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            \"vf_clip_param\": lambda: random.uniform(20000, 50000),\n",
    "            \"lr\": [5e-2, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(N_CPUS*HORIZON, 2*N_CPUS*HORIZON),\n",
    "        },\n",
    "        custom_explore_fn=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "# #      \"class registered in the tune registry.\")\n",
    "alg_run = \"DQN\"\n",
    "\n",
    "# HORIZON = 200\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"num_envs_per_worker\"] = 1  # number of parallel workers\n",
    "# config[\"num_gpus\"] = 0.1\n",
    "config[\"train_batch_size\"] = BATCH_SIZE * N_CPUS  # batch size\n",
    "config[\"sample_batch_size\"] = BATCH_SIZE  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [100, 100]})  # size of hidden layers in network\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "# config[\"n_step\"] = 2\n",
    "# config[\"noisy\"] = True\n",
    "# config[\"num_atoms\"] = 2\n",
    "\n",
    "# save the flow params for replay\n",
    "# flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "#                        indent=4)  # generating a string version of flow_params\n",
    "# config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "# config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "# create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "config[\"env\"] = \"CartPole-v1\"\n",
    "# Register as rllib env with Gym\n",
    "# register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(\"cart_pole_tests\", **{\n",
    "        \"run\": alg_run,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 5,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 5,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"episode_reward_mean\": 200,  # number of iterations to stop after\n",
    "        },\n",
    "        \"num_samples\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 11:21:55,584\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-03-10 11:21:55,602\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-03-10 11:21:55,614\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-03-10 11:21:55,621\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2020-03-10 11:21:55,699\tWARNING util.py:145 -- The `start_trial` operation took 0.10407447814941406 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.6/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,170\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,170\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58.171154: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58.198297: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58.198697: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c7cd00 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58.198717: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,199\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,199\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe8d7e3b438>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,412\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fe8d44c6c50>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,421\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fe8d4467ac8>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,514\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,520\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fe8d43792b0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,562\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fe8d432b3c8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:58,602\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fe8d42caa90>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,119\tDEBUG tf_policy.py:214 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,280\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,281\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,281\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,282\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,283\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,283\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,284\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,285\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,286\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,286\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,287\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,288\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,288\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fe8d7e3b6d8>}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,288\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fe8d7e3b438>}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,289\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fe8d7e1b588>}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,290\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fe8d7e3b2e8> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fe8d7e3b6d8>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,328\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:21:59,384\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 0}\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01,786\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01,787\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01.787343: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01.826082: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01.826263: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40c0f40 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01.826282: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01,826\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:01,826\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f0b0a4a2be0>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:02,066\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f0b08323438>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:02,076\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f0b08323320>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,055\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,056\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02.056782: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02.063002: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02.063180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e00e10 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02.063201: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,063\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,063\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fdf943e4ba8>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,130\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,131\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02.131284: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02.136200: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02.136368: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x481ecd0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02.136389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,136\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,137\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fb050629be0>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:02,199\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f0b082155c0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:02,254\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f0b081dae80>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:02,302\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f0b08166ef0>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,410\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fdf902621d0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,421\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fdf90262a20>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,479\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fb04c4a8438>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,490\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fb04c4a8358>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,574\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,588\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fdf901579e8>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,635\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fb04c39b5c0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,670\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fdf9010feb8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,705\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fb04c35fe80>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:02,765\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fb04c2ebef0>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:02,742\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fdf900b5208>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 100), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,380\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,382\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,383\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,385\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,386\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,388\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,389\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,391\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,393\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,395\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,397\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,398\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m 2020-03-10 11:22:03,401\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f0af862a390> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f0b0a4a2e80>}\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6485)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:03,652\tDEBUG tf_policy.py:214 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,030\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,032\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,033\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,035\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,036\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,038\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,039\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,041\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,042\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,044\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,045\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,047\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m 2020-03-10 11:22:04,050\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fb0367ad358> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fb050629e80>}\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6486)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,149\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,150\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,152\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,153\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,155\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,156\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,158\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,160\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,162\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,163\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,164\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,165\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,167\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fdf943e4a58> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fdf943e4e48>}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,261\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,261\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.046, max=0.011, mean=-0.023)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,261\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,262\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.046, max=0.011, mean=-0.023)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,262\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.046, max=0.011, mean=-0.023)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,262\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.046, max=0.011, mean=-0.023),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,262\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,328\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=0.018, max=0.028, mean=0.023)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,349\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.615),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((13,), dtype=bool, min=0.0, max=1.0, mean=0.077),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((13,), dtype=int64, min=1657737626.0, max=1657737626.0, mean=1657737626.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((13,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((13, 4), dtype=float32, min=-1.496, max=0.803, mean=-0.114),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((13, 4), dtype=float32, min=-1.496, max=0.803, mean=-0.1),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.615),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((13,), dtype=float32, min=0.0, max=1.0, mean=0.923),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((13, 2), dtype=float32, min=-0.11, max=0.627, mean=0.246),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((13,), dtype=int64, min=0.0, max=12.0, mean=6.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:22:04,582\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.52),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=228478984.0, max=1905706576.0, mean=725166944.39),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.497, max=2.414, mean=-0.009),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.497, max=2.073, mean=-0.008),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.95),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=-0.36, max=0.686, mean=0.067),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=43.0, mean=12.61),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-05\n",
      "  done: false\n",
      "  episode_len_mean: 20.625\n",
      "  episode_reward_max: 72.0\n",
      "  episode_reward_mean: 20.625\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 56\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 1200\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 2\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 2470.757\n",
      "    update_time_ms: 357.502\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.75555555555556\n",
      "    ram_util_percent: 50.75555555555556\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.1294671384983314\n",
      "    mean_inference_ms: 1.5378427709342326\n",
      "    mean_processing_ms: 0.19006031367292087\n",
      "  time_since_restore: 5.734468460083008\n",
      "  time_this_iter_s: 5.734468460083008\n",
      "  time_total_s: 5.734468460083008\n",
      "  timestamp: 1583835725\n",
      "  timesteps_since_restore: 1200\n",
      "  timesteps_this_iter: 1200\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 1\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m /usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.4/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 5 s, 1 iter, 1200 ts, 20.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,133\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 1200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.559),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=0.0, max=1798.0, mean=892.061),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.046),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-2.547, max=2.622, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.335, max=2.426, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,574\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,575\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,575\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,575\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,575\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,575\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:05,796\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 0.91259503,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 0.112132184,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': -1.0978618,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': -0.42091152,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-1.753, max=-0.31, mean=-1.098)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:06,369\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 2400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:07,868\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 4200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:09,151\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 6000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-10\n",
      "  done: false\n",
      "  episode_len_mean: 13.301470588235293\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: 13.301470588235293\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 136\n",
      "  episodes_total: 450\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 5.3451828956604\n",
      "        mean_q: 2.119241237640381\n",
      "        mean_td_error: -1.5850518941879272\n",
      "        min_q: -0.00666847825050354\n",
      "        model: {}\n",
      "    max_exploration: 0.41200000000000003\n",
      "    min_exploration: 0.41200000000000003\n",
      "    num_steps_sampled: 7800\n",
      "    num_steps_trained: 8800\n",
      "    num_target_updates: 13\n",
      "    opt_peak_throughput: 12876.351\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.693\n",
      "    sample_time_ms: 304.379\n",
      "    update_time_ms: 6.782\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.75\n",
      "    ram_util_percent: 53.650000000000006\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.08377455806630808\n",
      "    mean_inference_ms: 1.070544334275052\n",
      "    mean_processing_ms: 0.14042353458599946\n",
      "  time_since_restore: 10.895596265792847\n",
      "  time_this_iter_s: 1.1590938568115234\n",
      "  time_total_s: 10.895596265792847\n",
      "  timestamp: 1583835730\n",
      "  timesteps_since_restore: 7800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 7800\n",
      "  training_iteration: 5\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.4/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 10 s, 5 iter, 7800 ts, 13.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:10,325\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 7800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:11,512\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 9600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:12,904\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 11400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:14,100\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 13200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:15,250\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 15000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-16\n",
      "  done: false\n",
      "  episode_len_mean: 9.427083333333334\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.427083333333334\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 1376\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.51\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 14.944951057434082\n",
      "        mean_q: 8.580818176269531\n",
      "        mean_td_error: -1.7639209032058716\n",
      "        min_q: 0.03831864893436432\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 16800\n",
      "    num_steps_trained: 20800\n",
      "    num_target_updates: 28\n",
      "    opt_peak_throughput: 14411.924\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 61.836\n",
      "    sample_time_ms: 267.937\n",
      "    update_time_ms: 5.579\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.75\n",
      "    ram_util_percent: 54.4\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07500160679053545\n",
      "    mean_inference_ms: 0.967774342647771\n",
      "    mean_processing_ms: 0.13450335258743149\n",
      "  time_since_restore: 16.970306396484375\n",
      "  time_this_iter_s: 1.1690704822540283\n",
      "  time_total_s: 16.970306396484375\n",
      "  timestamp: 1583835736\n",
      "  timesteps_since_restore: 16800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 16800\n",
      "  training_iteration: 10\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 16 s, 10 iter, 16800 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:16,433\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 16800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:17,716\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 18600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:19,055\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 20400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:20,342\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 22200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-21\n",
      "  done: false\n",
      "  episode_len_mean: 9.4375\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.4375\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 2140\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.632\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 26.166950225830078\n",
      "        mean_q: 16.508502960205078\n",
      "        mean_td_error: -1.021681547164917\n",
      "        min_q: 0.02604454755783081\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 30400\n",
      "    num_target_updates: 40\n",
      "    opt_peak_throughput: 13881.179\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 66.009\n",
      "    sample_time_ms: 294.915\n",
      "    update_time_ms: 5.023\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.7\n",
      "    ram_util_percent: 54.45\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07370958790795802\n",
      "    mean_inference_ms: 0.9521876095878686\n",
      "    mean_processing_ms: 0.13527891350747026\n",
      "  time_since_restore: 22.014272212982178\n",
      "  time_this_iter_s: 1.1494548320770264\n",
      "  time_total_s: 22.014272212982178\n",
      "  timestamp: 1583835741\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 14\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 22 s, 14 iter, 24000 ts, 9.44 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:21,500\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 24000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:22,757\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 25800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:23,935\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 27600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:25,294\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 29400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-26\n",
      "  done: false\n",
      "  episode_len_mean: 9.463157894736842\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.463157894736842\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 2901\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.987\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 41.32019805908203\n",
      "        mean_q: 28.148290634155273\n",
      "        mean_td_error: -1.2878141403198242\n",
      "        min_q: -0.10918998718261719\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 31200\n",
      "    num_steps_trained: 40000\n",
      "    num_target_updates: 52\n",
      "    opt_peak_throughput: 13562.322\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.684\n",
      "    sample_time_ms: 291.39\n",
      "    update_time_ms: 5.423\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.4\n",
      "    ram_util_percent: 54.95\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.072599584286743\n",
      "    mean_inference_ms: 0.9390466780344973\n",
      "    mean_processing_ms: 0.13504056442796056\n",
      "  time_since_restore: 27.047483682632446\n",
      "  time_this_iter_s: 1.2581405639648438\n",
      "  time_total_s: 27.047483682632446\n",
      "  timestamp: 1583835746\n",
      "  timesteps_since_restore: 31200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 31200\n",
      "  training_iteration: 18\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 27 s, 18 iter, 31200 ts, 9.46 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:26,560\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 31200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:27,994\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 33000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:29,174\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 34800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:30,634\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 36600}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-31\n",
      "  done: false\n",
      "  episode_len_mean: 9.478947368421053\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.478947368421053\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 3665\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.719\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 60.56824493408203\n",
      "        mean_q: 42.748817443847656\n",
      "        mean_td_error: -1.0007596015930176\n",
      "        min_q: -1.9462337493896484\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 38400\n",
      "    num_steps_trained: 49600\n",
      "    num_target_updates: 64\n",
      "    opt_peak_throughput: 13624.113\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.773\n",
      "    sample_time_ms: 295.06\n",
      "    update_time_ms: 5.366\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.6\n",
      "    ram_util_percent: 55.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07239007842588333\n",
      "    mean_inference_ms: 0.9374649811445046\n",
      "    mean_processing_ms: 0.13593196157225568\n",
      "  time_since_restore: 32.31562900543213\n",
      "  time_this_iter_s: 1.2183058261871338\n",
      "  time_total_s: 32.31562900543213\n",
      "  timestamp: 1583835751\n",
      "  timesteps_since_restore: 38400\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 38400\n",
      "  training_iteration: 22\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 32 s, 22 iter, 38400 ts, 9.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:31,860\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 38400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:33,017\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 40200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:34,178\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 42000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:35,259\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 43200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:36,587\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 45000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-37\n",
      "  done: false\n",
      "  episode_len_mean: 9.48421052631579\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.48421052631579\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 4551\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.409\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 87.91532897949219\n",
      "        mean_q: 67.04931640625\n",
      "        mean_td_error: 2.433361530303955\n",
      "        min_q: -6.066165924072266\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 46800\n",
      "    num_steps_trained: 60800\n",
      "    num_target_updates: 78\n",
      "    opt_peak_throughput: 14182.128\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.835\n",
      "    sample_time_ms: 301.343\n",
      "    update_time_ms: 5.754\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.95\n",
      "    ram_util_percent: 55.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07234167824485031\n",
      "    mean_inference_ms: 0.936853401564858\n",
      "    mean_processing_ms: 0.13636477912853234\n",
      "  time_since_restore: 38.179582357406616\n",
      "  time_this_iter_s: 1.1673038005828857\n",
      "  time_total_s: 38.179582357406616\n",
      "  timestamp: 1583835757\n",
      "  timesteps_since_restore: 46800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 46800\n",
      "  training_iteration: 27\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 38 s, 27 iter, 46800 ts, 9.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:37,761\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 46800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:39,061\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 48600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:40,228\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 50400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:41,405\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 52200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:42,560\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 54000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-43\n",
      "  done: false\n",
      "  episode_len_mean: 9.473684210526315\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.473684210526315\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 5506\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.004\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 125.26506042480469\n",
      "        mean_q: 101.83381652832031\n",
      "        mean_td_error: 8.503986358642578\n",
      "        min_q: -12.712745666503906\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 55800\n",
      "    num_steps_trained: 72800\n",
      "    num_target_updates: 93\n",
      "    opt_peak_throughput: 14284.645\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.888\n",
      "    sample_time_ms: 257.809\n",
      "    update_time_ms: 5.05\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.8\n",
      "    ram_util_percent: 55.5\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07140975049083062\n",
      "    mean_inference_ms: 0.9234831441023155\n",
      "    mean_processing_ms: 0.13481371421339367\n",
      "  time_since_restore: 44.114213705062866\n",
      "  time_this_iter_s: 1.158597469329834\n",
      "  time_total_s: 44.114213705062866\n",
      "  timestamp: 1583835763\n",
      "  timesteps_since_restore: 55800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 55800\n",
      "  training_iteration: 32\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 44 s, 32 iter, 55800 ts, 9.47 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:43,727\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 55800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:44,895\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 57600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:46,056\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 59400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:47,212\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 61200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:48,388\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 63000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 9.457894736842105\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.457894736842105\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 6458\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.944\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 182.34188842773438\n",
      "        mean_q: 159.5149383544922\n",
      "        mean_td_error: 23.521326065063477\n",
      "        min_q: -22.109268188476562\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 64800\n",
      "    num_steps_trained: 84800\n",
      "    num_target_updates: 108\n",
      "    opt_peak_throughput: 14299.974\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.733\n",
      "    sample_time_ms: 259.143\n",
      "    update_time_ms: 5.031\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.1\n",
      "    ram_util_percent: 55.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07036176558936182\n",
      "    mean_inference_ms: 0.9083060197431296\n",
      "    mean_processing_ms: 0.13331396431800793\n",
      "  time_since_restore: 49.912394762039185\n",
      "  time_this_iter_s: 1.1626176834106445\n",
      "  time_total_s: 49.912394762039185\n",
      "  timestamp: 1583835769\n",
      "  timesteps_since_restore: 64800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 64800\n",
      "  training_iteration: 37\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 49 s, 37 iter, 64800 ts, 9.46 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:49,559\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 64800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:50,722\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 66600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:51,871\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 68400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:53,167\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 70200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:54,224\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 71400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-22-55\n",
      "  done: false\n",
      "  episode_len_mean: 9.397905759162304\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.397905759162304\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 7352\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.092\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 241.3695068359375\n",
      "        mean_q: 218.892578125\n",
      "        mean_td_error: 33.48513412475586\n",
      "        min_q: -32.722877502441406\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 73200\n",
      "    num_steps_trained: 96000\n",
      "    num_target_updates: 122\n",
      "    opt_peak_throughput: 13312.895\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 68.544\n",
      "    sample_time_ms: 288.066\n",
      "    update_time_ms: 4.97\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.15\n",
      "    ram_util_percent: 55.9\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07012801085039856\n",
      "    mean_inference_ms: 0.9061738544136794\n",
      "    mean_processing_ms: 0.13312558083210352\n",
      "  time_since_restore: 55.721203565597534\n",
      "  time_this_iter_s: 1.1662120819091797\n",
      "  time_total_s: 55.721203565597534\n",
      "  timestamp: 1583835775\n",
      "  timesteps_since_restore: 73200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 73200\n",
      "  training_iteration: 42\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 55 s, 42 iter, 73200 ts, 9.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:55,398\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 73200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:56,567\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 75000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:57,617\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 76200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:22:59,019\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 78000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-00\n",
      "  done: false\n",
      "  episode_len_mean: 9.43157894736842\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.43157894736842\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 8050\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 61.363\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 289.6286926269531\n",
      "        mean_q: 261.11407470703125\n",
      "        mean_td_error: 40.2161865234375\n",
      "        min_q: -41.927879333496094\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 79800\n",
      "    num_steps_trained: 104800\n",
      "    num_target_updates: 133\n",
      "    opt_peak_throughput: 13037.181\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.795\n",
      "    sample_time_ms: 323.565\n",
      "    update_time_ms: 6.303\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.65\n",
      "    ram_util_percent: 56.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0708353142083598\n",
      "    mean_inference_ms: 0.9177139062719893\n",
      "    mean_processing_ms: 0.13472415950511488\n",
      "  time_since_restore: 60.761587142944336\n",
      "  time_this_iter_s: 1.4487037658691406\n",
      "  time_total_s: 60.761587142944336\n",
      "  timestamp: 1583835780\n",
      "  timesteps_since_restore: 79800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 79800\n",
      "  training_iteration: 46\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 60 s, 46 iter, 79800 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:00,475\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 79800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:01,643\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 81600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:02,805\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 83400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:03,955\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 85200}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,582\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-2.116, max=1.335, mean=-0.206)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,582\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,582\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-2.116, max=1.335, mean=-0.206)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,582\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-2.116, max=1.335, mean=-0.206)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,583\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-2.116, max=1.335, mean=-0.206),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,583\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,584\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=-52.488, max=344.84, mean=146.176)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,587\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((9,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((9,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((9,), dtype=bool, min=0.0, max=1.0, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((9,), dtype=int64, min=796554504.0, max=796554504.0, mean=796554504.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((9,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((9, 4), dtype=float32, min=-2.789, max=1.727, mean=-0.146),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((9, 4), dtype=float32, min=-2.447, max=1.531, mean=-0.115),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((9,), dtype=int64, min=0.0, max=1.0, mean=0.889),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((9,), dtype=float32, min=0.0, max=1.0, mean=0.889),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((9, 2), dtype=float32, min=-52.489, max=344.841, mean=139.813),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((9,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((9,), dtype=int64, min=0.0, max=8.0, mean=4.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((9,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((9,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,653\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.995),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.105),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=257688690.0, max=1844172143.0, mean=1125377046.33),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-3.097, max=2.002, mean=-0.14),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.752, max=1.806, mean=-0.11),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.89),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=-52.489, max=344.841, mean=141.351),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=10.0, mean=4.175),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=143.0, max=143.0, mean=143.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:23:04,907\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:05,343\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 87000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,059\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,060\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,069\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 360.28314,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 333.14,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': 51.137463,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': -55.46959,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-416.391, max=359.283, mean=51.137)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,459\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.979),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=92.0, max=49986.0, mean=27300.066),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.22),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-3.312, max=2.186, mean=-0.151),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.964, max=1.99, mean=-0.121),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=0.211, max=0.987, mean=0.269)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 9.542105263157895\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.542105263157895\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 9004\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.868\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 365.4313049316406\n",
      "        mean_q: 342.37054443359375\n",
      "        mean_td_error: 59.25404739379883\n",
      "        min_q: -56.459503173828125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 88800\n",
      "    num_steps_trained: 116800\n",
      "    num_target_updates: 148\n",
      "    opt_peak_throughput: 13143.231\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.959\n",
      "    sample_time_ms: 273.244\n",
      "    update_time_ms: 5.328\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.8\n",
      "    ram_util_percent: 56.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07045247939572113\n",
      "    mean_inference_ms: 0.9114755911352967\n",
      "    mean_processing_ms: 0.13407806527769556\n",
      "  time_since_restore: 66.7860336303711\n",
      "  time_this_iter_s: 1.1791670322418213\n",
      "  time_total_s: 66.7860336303711\n",
      "  timestamp: 1583835786\n",
      "  timesteps_since_restore: 88800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 88800\n",
      "  training_iteration: 51\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 66 s, 51 iter, 88800 ts, 9.54 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:06,530\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 88800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:07,696\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 90600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:08,872\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 92400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:10,038\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 94200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:11,201\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 96000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-12\n",
      "  done: false\n",
      "  episode_len_mean: 9.418848167539267\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.418848167539267\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 9957\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.645\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 440.58624267578125\n",
      "        mean_q: 421.3455810546875\n",
      "        mean_td_error: 90.06993103027344\n",
      "        min_q: -71.08256530761719\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 97800\n",
      "    num_steps_trained: 128800\n",
      "    num_target_updates: 163\n",
      "    opt_peak_throughput: 14376.856\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.704\n",
      "    sample_time_ms: 258.472\n",
      "    update_time_ms: 5.094\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.85\n",
      "    ram_util_percent: 56.25\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06987643821377229\n",
      "    mean_inference_ms: 0.9027426166553688\n",
      "    mean_processing_ms: 0.1331123462071764\n",
      "  time_since_restore: 72.59752297401428\n",
      "  time_this_iter_s: 1.1641592979431152\n",
      "  time_total_s: 72.59752297401428\n",
      "  timestamp: 1583835792\n",
      "  timesteps_since_restore: 97800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 97800\n",
      "  training_iteration: 56\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 72 s, 56 iter, 97800 ts, 9.42 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:12,373\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 97800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:13,539\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 99600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:14,713\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 101400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:15,872\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 103200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:17,224\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 105000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-18\n",
      "  done: false\n",
      "  episode_len_mean: 9.309278350515465\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.309278350515465\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 194\n",
      "  episodes_total: 10917\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.721\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 508.80047607421875\n",
      "        mean_q: 492.071044921875\n",
      "        mean_td_error: 111.83800506591797\n",
      "        min_q: -84.63909912109375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 106800\n",
      "    num_steps_trained: 140800\n",
      "    num_target_updates: 178\n",
      "    opt_peak_throughput: 13175.031\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.438\n",
      "    sample_time_ms: 289.449\n",
      "    update_time_ms: 5.288\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.94999999999999\n",
      "    ram_util_percent: 56.650000000000006\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06994470017889741\n",
      "    mean_inference_ms: 0.9016829904476767\n",
      "    mean_processing_ms: 0.13317688171218525\n",
      "  time_since_restore: 78.79390692710876\n",
      "  time_this_iter_s: 1.3735380172729492\n",
      "  time_total_s: 78.79390692710876\n",
      "  timestamp: 1583835798\n",
      "  timesteps_since_restore: 106800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 106800\n",
      "  training_iteration: 61\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 78 s, 61 iter, 106800 ts, 9.31 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:18,606\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 106800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:19,933\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 108600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:20,959\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 109800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:22,328\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 111600}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 9.43157894736842\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 9.43157894736842\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 11619\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.59\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 553.946533203125\n",
      "        mean_q: 533.0164794921875\n",
      "        mean_td_error: 131.31256103515625\n",
      "        min_q: -93.66677856445312\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 113400\n",
      "    num_steps_trained: 149600\n",
      "    num_target_updates: 189\n",
      "    opt_peak_throughput: 12781.526\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.501\n",
      "    sample_time_ms: 326.596\n",
      "    update_time_ms: 5.549\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.6\n",
      "    ram_util_percent: 56.650000000000006\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07049070108067636\n",
      "    mean_inference_ms: 0.9108107357146213\n",
      "    mean_processing_ms: 0.134575232613856\n",
      "  time_since_restore: 83.90027141571045\n",
      "  time_this_iter_s: 1.398752212524414\n",
      "  time_total_s: 83.90027141571045\n",
      "  timestamp: 1583835803\n",
      "  timesteps_since_restore: 113400\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 113400\n",
      "  training_iteration: 65\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 83 s, 65 iter, 113400 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:23,744\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 113400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:24,972\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 115200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:26,360\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 117000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:27,605\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 118800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-28\n",
      "  done: false\n",
      "  episode_len_mean: 9.510526315789473\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.510526315789473\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 12383\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 59.673\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 597.7247314453125\n",
      "        mean_q: 572.800048828125\n",
      "        mean_td_error: 120.4338150024414\n",
      "        min_q: -102.01943969726562\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 120600\n",
      "    num_steps_trained: 159200\n",
      "    num_target_updates: 201\n",
      "    opt_peak_throughput: 13406.313\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.718\n",
      "    sample_time_ms: 292.025\n",
      "    update_time_ms: 5.187\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.3\n",
      "    ram_util_percent: 56.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07047146619745977\n",
      "    mean_inference_ms: 0.9112988972833717\n",
      "    mean_processing_ms: 0.13482702405673105\n",
      "  time_since_restore: 88.97319054603577\n",
      "  time_this_iter_s: 1.2243387699127197\n",
      "  time_total_s: 88.97319054603577\n",
      "  timestamp: 1583835808\n",
      "  timesteps_since_restore: 120600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 120600\n",
      "  training_iteration: 69\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 88 s, 69 iter, 120600 ts, 9.51 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:28,843\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 120600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:30,203\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 122400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:31,364\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 124200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:32,532\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 126000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-33\n",
      "  done: false\n",
      "  episode_len_mean: 9.413612565445026\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 9.413612565445026\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 13144\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 637.3055419921875\n",
      "        mean_q: 611.4876708984375\n",
      "        mean_td_error: 140.4733428955078\n",
      "        min_q: -109.17263793945312\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 127800\n",
      "    num_steps_trained: 168800\n",
      "    num_target_updates: 213\n",
      "    opt_peak_throughput: 12777.102\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 68.029\n",
      "    sample_time_ms: 265.907\n",
      "    update_time_ms: 5.135\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.05000000000001\n",
      "    ram_util_percent: 56.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0703355682304903\n",
      "    mean_inference_ms: 0.9094776900615941\n",
      "    mean_processing_ms: 0.13458518935815686\n",
      "  time_since_restore: 93.9986081123352\n",
      "  time_this_iter_s: 1.3555872440338135\n",
      "  time_total_s: 93.9986081123352\n",
      "  timestamp: 1583835813\n",
      "  timesteps_since_restore: 127800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 127800\n",
      "  training_iteration: 73\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 93 s, 73 iter, 127800 ts, 9.41 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:33,903\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 127800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:35,256\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 129600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:36,419\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 131400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:37,592\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 133200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-38\n",
      "  done: false\n",
      "  episode_len_mean: 9.582887700534759\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.582887700534759\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 13903\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.529\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 671.2031860351562\n",
      "        mean_q: 650.2168579101562\n",
      "        mean_td_error: 154.8153076171875\n",
      "        min_q: -114.88177490234375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 178400\n",
      "    num_target_updates: 225\n",
      "    opt_peak_throughput: 14406.899\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.108\n",
      "    sample_time_ms: 283.437\n",
      "    update_time_ms: 5.558\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.3\n",
      "    ram_util_percent: 56.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07033406016648446\n",
      "    mean_inference_ms: 0.9095792438413592\n",
      "    mean_processing_ms: 0.13463084824749927\n",
      "  time_since_restore: 99.01346468925476\n",
      "  time_this_iter_s: 1.345240592956543\n",
      "  time_total_s: 99.01346468925476\n",
      "  timestamp: 1583835818\n",
      "  timesteps_since_restore: 135000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 77\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 99 s, 77 iter, 135000 ts, 9.58 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:38,945\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 135000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:40,349\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 136800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:41,611\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 138600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:42,787\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 140400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-43\n",
      "  done: false\n",
      "  episode_len_mean: 9.434554973821989\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.434554973821989\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 14668\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.284\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 699.9334106445312\n",
      "        mean_q: 656.9994506835938\n",
      "        mean_td_error: 147.2633819580078\n",
      "        min_q: -119.10012817382812\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 142200\n",
      "    num_steps_trained: 188000\n",
      "    num_target_updates: 237\n",
      "    opt_peak_throughput: 13725.844\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.191\n",
      "    sample_time_ms: 278.786\n",
      "    update_time_ms: 5.041\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.25\n",
      "    ram_util_percent: 56.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07024915105476434\n",
      "    mean_inference_ms: 0.9088671982859269\n",
      "    mean_processing_ms: 0.13452704574132135\n",
      "  time_since_restore: 104.03684306144714\n",
      "  time_this_iter_s: 1.201073169708252\n",
      "  time_total_s: 104.03684306144714\n",
      "  timestamp: 1583835823\n",
      "  timesteps_since_restore: 142200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 142200\n",
      "  training_iteration: 81\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 104 s, 81 iter, 142200 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:44,001\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 142200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:45,310\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 144000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:46,478\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 145800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:47,774\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 147600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:48,956\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 149400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-50\n",
      "  done: false\n",
      "  episode_len_mean: 9.432291666666666\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.432291666666666\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 15622\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.187\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 729.20458984375\n",
      "        mean_q: 696.3929443359375\n",
      "        mean_td_error: 186.5737762451172\n",
      "        min_q: -122.35415649414062\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 151200\n",
      "    num_steps_trained: 200000\n",
      "    num_target_updates: 252\n",
      "    opt_peak_throughput: 13748.819\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.749\n",
      "    sample_time_ms: 270.363\n",
      "    update_time_ms: 5.062\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.3\n",
      "    ram_util_percent: 57.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0700752768974336\n",
      "    mean_inference_ms: 0.9061473796189669\n",
      "    mean_processing_ms: 0.13420837771528857\n",
      "  time_since_restore: 110.12501382827759\n",
      "  time_this_iter_s: 1.1568410396575928\n",
      "  time_total_s: 110.12501382827759\n",
      "  timestamp: 1583835830\n",
      "  timesteps_since_restore: 151200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 151200\n",
      "  training_iteration: 86\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 110 s, 86 iter, 151200 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:50,122\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 151200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:51,305\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 153000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:52,461\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 154800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:53,626\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 156600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:54,980\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 158400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-23-56\n",
      "  done: false\n",
      "  episode_len_mean: 9.424083769633508\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.424083769633508\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 16571\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.224\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 749.8992919921875\n",
      "        mean_q: 717.9241943359375\n",
      "        mean_td_error: 171.99081420898438\n",
      "        min_q: -123.06228637695312\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 160200\n",
      "    num_steps_trained: 212000\n",
      "    num_target_updates: 267\n",
      "    opt_peak_throughput: 13283.807\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 69.998\n",
      "    sample_time_ms: 271.817\n",
      "    update_time_ms: 5.128\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.9\n",
      "    ram_util_percent: 57.2\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06980614494741962\n",
      "    mean_inference_ms: 0.9027162424474543\n",
      "    mean_processing_ms: 0.1337858141309027\n",
      "  time_since_restore: 116.18286919593811\n",
      "  time_this_iter_s: 1.2241578102111816\n",
      "  time_total_s: 116.18286919593811\n",
      "  timestamp: 1583835836\n",
      "  timesteps_since_restore: 160200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 160200\n",
      "  training_iteration: 91\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 116 s, 91 iter, 160200 ts, 9.42 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:56,212\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 160200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:57,590\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 162000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:58,651\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 163200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:23:59,989\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 165000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-01\n",
      "  done: false\n",
      "  episode_len_mean: 9.5\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.5\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 17270\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 63.073\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 762.2564697265625\n",
      "        mean_q: 736.826171875\n",
      "        mean_td_error: 202.38182067871094\n",
      "        min_q: -122.37527465820312\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 166800\n",
      "    num_steps_trained: 220800\n",
      "    num_target_updates: 278\n",
      "    opt_peak_throughput: 12683.751\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 69.427\n",
      "    sample_time_ms: 312.765\n",
      "    update_time_ms: 5.082\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.3\n",
      "    ram_util_percent: 57.5\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07011333058164879\n",
      "    mean_inference_ms: 0.9072568443657422\n",
      "    mean_processing_ms: 0.13437476176120206\n",
      "  time_since_restore: 121.22476077079773\n",
      "  time_this_iter_s: 1.2776761054992676\n",
      "  time_total_s: 121.22476077079773\n",
      "  timestamp: 1583835841\n",
      "  timesteps_since_restore: 166800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 166800\n",
      "  training_iteration: 95\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 121 s, 95 iter, 166800 ts, 9.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:01,282\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 166800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:02,598\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 168600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:04,042\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 170400}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,908\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-1.235, max=0.791, mean=-0.127)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,908\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,908\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-1.235, max=0.791, mean=-0.127)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,908\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-1.235, max=0.791, mean=-0.127)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,909\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-1.235, max=0.791, mean=-0.127),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,909\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,911\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=-121.091, max=769.37, mean=324.14)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:04,917\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((8,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((8,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((8,), dtype=bool, min=0.0, max=1.0, mean=0.125),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((8,), dtype=int64, min=1283572991.0, max=1283572991.0, mean=1283572991.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((8,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((8, 4), dtype=float32, min=-2.547, max=1.576, mean=-0.148),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((8, 4), dtype=float32, min=-2.207, max=1.38, mean=-0.116),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((8,), dtype=int64, min=0.0, max=1.0, mean=0.875),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((8,), dtype=float32, min=0.0, max=1.0, mean=0.875),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((8, 2), dtype=float32, min=-121.091, max=769.37, mean=323.393),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((8,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((8,), dtype=int64, min=0.0, max=7.0, mean=3.5),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((8,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((8,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:05,080\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.105),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=91482950.0, max=1972846713.0, mean=1044186195.595),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-3.121, max=2.002, mean=-0.145),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.774, max=1.806, mean=-0.114),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=-121.091, max=769.37, mean=323.383),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=9.0, mean=4.16),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=286.0, max=286.0, mean=286.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:05,255\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 172200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:24:05,266\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-06\n",
      "  done: false\n",
      "  episode_len_mean: 9.413612565445026\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.413612565445026\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 18034\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.11\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 771.4285888671875\n",
      "        mean_q: 745.89794921875\n",
      "        mean_td_error: 191.95101928710938\n",
      "        min_q: -120.59622192382812\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 230400\n",
      "    num_target_updates: 290\n",
      "    opt_peak_throughput: 14007.996\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.929\n",
      "    sample_time_ms: 290.533\n",
      "    update_time_ms: 5.145\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.7\n",
      "    ram_util_percent: 57.45\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07022498703613483\n",
      "    mean_inference_ms: 0.9086435803108883\n",
      "    mean_processing_ms: 0.13456412749106422\n",
      "  time_since_restore: 126.36931037902832\n",
      "  time_this_iter_s: 1.1844794750213623\n",
      "  time_total_s: 126.36931037902832\n",
      "  timestamp: 1583835846\n",
      "  timesteps_since_restore: 174000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 99\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 126 s, 99 iter, 174000 ts, 9.41 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,448\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 174000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,815\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,816\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,816\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,816\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:06,824\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 772.0846,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 747.6047,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': 185.16942,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': -120.42163,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-892.734, max=771.085, mean=185.169)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:07,276\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.974),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=108.0, max=49907.0, mean=22447.825),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.268),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-3.302, max=3.119, mean=-0.164),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.955, max=2.771, mean=-0.132),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=0.137, max=0.999, mean=0.159)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:07,844\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 175800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:08,868\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 177000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:10,254\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 178800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 9.32642487046632\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.32642487046632\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 18736\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 64.71\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 777.647216796875\n",
      "        mean_q: 760.001220703125\n",
      "        mean_td_error: 237.2041015625\n",
      "        min_q: -118.43838500976562\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 180600\n",
      "    num_steps_trained: 239200\n",
      "    num_target_updates: 301\n",
      "    opt_peak_throughput: 12362.923\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.436\n",
      "    sample_time_ms: 324.946\n",
      "    update_time_ms: 5.679\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.55000000000001\n",
      "    ram_util_percent: 57.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07050735998210148\n",
      "    mean_inference_ms: 0.9142014580764115\n",
      "    mean_processing_ms: 0.13528708890998717\n",
      "  time_since_restore: 131.47414565086365\n",
      "  time_this_iter_s: 1.3251516819000244\n",
      "  time_total_s: 131.47414565086365\n",
      "  timestamp: 1583835851\n",
      "  timesteps_since_restore: 180600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 180600\n",
      "  training_iteration: 103\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 131 s, 103 iter, 180600 ts, 9.33 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:11,587\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 180600}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:12,759\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 182400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:13,953\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 184200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:15,226\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 186000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:16,401\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 187800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-17\n",
      "  done: false\n",
      "  episode_len_mean: 9.40625\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.40625\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 19689\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 781.4708251953125\n",
      "        mean_q: 748.0828857421875\n",
      "        mean_td_error: 177.0096893310547\n",
      "        min_q: -114.16650390625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 189600\n",
      "    num_steps_trained: 251200\n",
      "    num_target_updates: 316\n",
      "    opt_peak_throughput: 13979.912\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.934\n",
      "    sample_time_ms: 269.65\n",
      "    update_time_ms: 5.949\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.599999999999994\n",
      "    ram_util_percent: 57.7\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07027914813917087\n",
      "    mean_inference_ms: 0.911177569558224\n",
      "    mean_processing_ms: 0.13493464275834857\n",
      "  time_since_restore: 137.41878151893616\n",
      "  time_this_iter_s: 1.159776210784912\n",
      "  time_total_s: 137.41878151893616\n",
      "  timestamp: 1583835857\n",
      "  timesteps_since_restore: 189600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 189600\n",
      "  training_iteration: 108\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 137 s, 108 iter, 189600 ts, 9.41 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:17,569\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 189600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:18,719\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 191400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:19,942\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 193200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:21,310\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 195000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:22,560\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 196800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-23\n",
      "  done: false\n",
      "  episode_len_mean: 9.304123711340207\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.304123711340207\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 194\n",
      "  episodes_total: 20646\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 64.06\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 783.950439453125\n",
      "        mean_q: 754.0390625\n",
      "        mean_td_error: 206.95516967773438\n",
      "        min_q: -109.16796875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 198600\n",
      "    num_steps_trained: 263200\n",
      "    num_target_updates: 331\n",
      "    opt_peak_throughput: 12488.205\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 68.82\n",
      "    sample_time_ms: 281.994\n",
      "    update_time_ms: 5.825\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.4\n",
      "    ram_util_percent: 57.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07013702257164181\n",
      "    mean_inference_ms: 0.909231308149344\n",
      "    mean_processing_ms: 0.13479624761853504\n",
      "  time_since_restore: 143.5860471725464\n",
      "  time_this_iter_s: 1.2076802253723145\n",
      "  time_total_s: 143.5860471725464\n",
      "  timestamp: 1583835863\n",
      "  timesteps_since_restore: 198600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 198600\n",
      "  training_iteration: 113\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 143 s, 113 iter, 198600 ts, 9.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:23,778\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 198600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:25,192\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 200400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:26,503\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 202200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:27,771\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 204000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 9.523809523809524\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 9.523809523809524\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 21408\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.397\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 783.9129028320312\n",
      "        mean_q: 753.591552734375\n",
      "        mean_td_error: 175.65765380859375\n",
      "        min_q: -104.365966796875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 205800\n",
      "    num_steps_trained: 272800\n",
      "    num_target_updates: 343\n",
      "    opt_peak_throughput: 13245.763\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 68.533\n",
      "    sample_time_ms: 307.053\n",
      "    update_time_ms: 5.338\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.15\n",
      "    ram_util_percent: 58.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07028333087787106\n",
      "    mean_inference_ms: 0.9108506481760488\n",
      "    mean_processing_ms: 0.1350889106679233\n",
      "  time_since_restore: 148.94654059410095\n",
      "  time_this_iter_s: 1.3865389823913574\n",
      "  time_total_s: 148.94654059410095\n",
      "  timestamp: 1583835869\n",
      "  timesteps_since_restore: 205800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 205800\n",
      "  training_iteration: 117\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 148 s, 117 iter, 205800 ts, 9.52 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:29,166\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 205800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:30,474\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 207600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:31,649\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 209400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:33,120\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 211200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-34\n",
      "  done: false\n",
      "  episode_len_mean: 9.497354497354497\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.497354497354497\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 22172\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.636\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 782.8082275390625\n",
      "        mean_q: 743.8367919921875\n",
      "        mean_td_error: 171.67489624023438\n",
      "        min_q: -99.37521362304688\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 213000\n",
      "    num_steps_trained: 282400\n",
      "    num_target_updates: 355\n",
      "    opt_peak_throughput: 12772.141\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.632\n",
      "    sample_time_ms: 297.898\n",
      "    update_time_ms: 6.094\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.3\n",
      "    ram_util_percent: 58.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0705046002818039\n",
      "    mean_inference_ms: 0.913338155741142\n",
      "    mean_processing_ms: 0.13543926659664615\n",
      "  time_since_restore: 154.23670721054077\n",
      "  time_this_iter_s: 1.362410068511963\n",
      "  time_total_s: 154.23670721054077\n",
      "  timestamp: 1583835874\n",
      "  timesteps_since_restore: 213000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 213000\n",
      "  training_iteration: 121\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 154 s, 121 iter, 213000 ts, 9.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:34,491\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 213000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:35,838\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 214800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:37,002\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 216600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:38,184\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 218400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:39,351\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 220200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-40\n",
      "  done: false\n",
      "  episode_len_mean: 9.304123711340207\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.304123711340207\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 194\n",
      "  episodes_total: 23126\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.9\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 782.6568603515625\n",
      "        mean_q: 755.9154052734375\n",
      "        mean_td_error: 197.1182861328125\n",
      "        min_q: -93.20407104492188\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 294400\n",
      "    num_target_updates: 370\n",
      "    opt_peak_throughput: 14059.789\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.219\n",
      "    sample_time_ms: 259.897\n",
      "    update_time_ms: 5.08\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.349999999999994\n",
      "    ram_util_percent: 58.2\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07030682352019876\n",
      "    mean_inference_ms: 0.9107713410358024\n",
      "    mean_processing_ms: 0.13512205233685629\n",
      "  time_since_restore: 160.2399241924286\n",
      "  time_this_iter_s: 1.167273759841919\n",
      "  time_total_s: 160.2399241924286\n",
      "  timestamp: 1583835880\n",
      "  timesteps_since_restore: 222000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 126\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 160 s, 126 iter, 222000 ts, 9.3 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:40,526\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 222000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:41,689\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 223800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:42,863\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 225600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:44,167\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 227400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:45,344\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 229200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-46\n",
      "  done: false\n",
      "  episode_len_mean: 9.395833333333334\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.395833333333334\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 24077\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.454\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 780.6466064453125\n",
      "        mean_q: 757.6021118164062\n",
      "        mean_td_error: 190.11053466796875\n",
      "        min_q: -86.59555053710938\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 231000\n",
      "    num_steps_trained: 306400\n",
      "    num_target_updates: 385\n",
      "    opt_peak_throughput: 13924.255\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.558\n",
      "    sample_time_ms: 288.26\n",
      "    update_time_ms: 5.357\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5\n",
      "    ram_util_percent: 58.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07022117827294011\n",
      "    mean_inference_ms: 0.9097356524541954\n",
      "    mean_processing_ms: 0.13498790125735127\n",
      "  time_since_restore: 166.36107802391052\n",
      "  time_this_iter_s: 1.3258788585662842\n",
      "  time_total_s: 166.36107802391052\n",
      "  timestamp: 1583835886\n",
      "  timesteps_since_restore: 231000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 231000\n",
      "  training_iteration: 131\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 166 s, 131 iter, 231000 ts, 9.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:46,679\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 231000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:47,866\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 232800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:49,039\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 234600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:50,210\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 236400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:51,622\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 238200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-52\n",
      "  done: false\n",
      "  episode_len_mean: 9.416666666666666\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.416666666666666\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 25032\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.231\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 778.7283325195312\n",
      "        mean_q: 745.4337768554688\n",
      "        mean_td_error: 189.35137939453125\n",
      "        min_q: -79.92587280273438\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 318400\n",
      "    num_target_updates: 400\n",
      "    opt_peak_throughput: 13282.109\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.103\n",
      "    sample_time_ms: 295.051\n",
      "    update_time_ms: 5.662\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.45\n",
      "    ram_util_percent: 58.5\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07018607792296297\n",
      "    mean_inference_ms: 0.9098263057116766\n",
      "    mean_processing_ms: 0.1349361802146273\n",
      "  time_since_restore: 172.643630027771\n",
      "  time_this_iter_s: 1.3640127182006836\n",
      "  time_total_s: 172.643630027771\n",
      "  timestamp: 1583835892\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 136\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 172 s, 136 iter, 240000 ts, 9.42 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:52,994\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 240000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:54,153\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 241800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:55,582\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 243600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:56,765\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 245400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-24-58\n",
      "  done: false\n",
      "  episode_len_mean: 9.544973544973544\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.544973544973544\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 25790\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 60.401\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 775.681884765625\n",
      "        mean_q: 746.3290405273438\n",
      "        mean_td_error: 206.99879455566406\n",
      "        min_q: -74.03817749023438\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 247200\n",
      "    num_steps_trained: 328000\n",
      "    num_target_updates: 412\n",
      "    opt_peak_throughput: 13244.822\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.575\n",
      "    sample_time_ms: 297.976\n",
      "    update_time_ms: 5.192\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.95\n",
      "    ram_util_percent: 58.55\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07025241371503196\n",
      "    mean_inference_ms: 0.9103791080823186\n",
      "    mean_processing_ms: 0.13503555906189632\n",
      "  time_since_restore: 177.7436237335205\n",
      "  time_this_iter_s: 1.3416450023651123\n",
      "  time_total_s: 177.7436237335205\n",
      "  timestamp: 1583835898\n",
      "  timesteps_since_restore: 247200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 247200\n",
      "  training_iteration: 140\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 177 s, 140 iter, 247200 ts, 9.54 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:58,144\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 247200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:24:59,346\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 248400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:00,640\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 250200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:01,824\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 252000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:02,987\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 253800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-04\n",
      "  done: false\n",
      "  episode_len_mean: 9.631016042780749\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.631016042780749\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 26674\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.627\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 772.82177734375\n",
      "        mean_q: 745.9703369140625\n",
      "        mean_td_error: 220.99261474609375\n",
      "        min_q: -66.74688720703125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 255600\n",
      "    num_steps_trained: 339200\n",
      "    num_target_updates: 426\n",
      "    opt_peak_throughput: 13882.477\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.197\n",
      "    sample_time_ms: 268.034\n",
      "    update_time_ms: 5.225\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.55\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0703201872143236\n",
      "    mean_inference_ms: 0.9116262008165451\n",
      "    mean_processing_ms: 0.13513311425203411\n",
      "  time_since_restore: 183.82815504074097\n",
      "  time_this_iter_s: 1.2631499767303467\n",
      "  time_total_s: 183.82815504074097\n",
      "  timestamp: 1583835904\n",
      "  timesteps_since_restore: 255600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 255600\n",
      "  training_iteration: 145\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 183 s, 145 iter, 255600 ts, 9.63 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:04,266\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 255600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:05,426\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 257400}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,438\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-1.152, max=0.755, mean=-0.102),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,439\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,440\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=-64.591, max=771.708, mean=353.559)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,441\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-1.45, max=0.951, mean=-0.13)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,441\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,441\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-1.45, max=0.951, mean=-0.13)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,441\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-1.45, max=0.951, mean=-0.13)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,448\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((6,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((6,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((6,), dtype=bool, min=0.0, max=1.0, mean=0.167),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((6,), dtype=int64, min=1373875899.0, max=1373875899.0, mean=1373875899.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((6,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((6, 4), dtype=float32, min=-3.069, max=1.93, mean=-0.214),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((6, 4), dtype=float32, min=-2.724, max=1.734, mean=-0.18),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((6,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((6,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((6, 2), dtype=float32, min=-64.591, max=771.708, mean=353.559),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((6,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((6,), dtype=int64, min=4.0, max=9.0, mean=6.5),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((6,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((6,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,661\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.94),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.105),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=15288555.0, max=1866385536.0, mean=882349336.2),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-3.106, max=2.817, mean=-0.13),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.974, max=2.478, mean=-0.103),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.845),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=-194.032, max=771.708, mean=340.134),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=10.0, mean=4.25),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=429.0, max=429.0, mean=429.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:25:05,829\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:06,631\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 259200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,362\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,362\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,363\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,364\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,364\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,364\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,373\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 770.79,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 761.6676,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': 221.69308,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': -62.478363,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-833.498, max=769.79, mean=221.693)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,744\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.976),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=26.0, max=49979.0, mean=22172.371),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.276),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-3.325, max=2.194, mean=-0.17),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.977, max=1.998, mean=-0.138),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=0.136, max=1.0, mean=0.157)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:07,812\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 261000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:09,187\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 262800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-10\n",
      "  done: false\n",
      "  episode_len_mean: 9.403141361256544\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.403141361256544\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 27629\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 61.473\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 769.0233764648438\n",
      "        mean_q: 753.09375\n",
      "        mean_td_error: 192.6201934814453\n",
      "        min_q: -59.0396728515625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 264600\n",
      "    num_steps_trained: 351200\n",
      "    num_target_updates: 441\n",
      "    opt_peak_throughput: 13013.74\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 72.008\n",
      "    sample_time_ms: 279.548\n",
      "    update_time_ms: 4.93\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.4\n",
      "    ram_util_percent: 58.7\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07023248443536934\n",
      "    mean_inference_ms: 0.9101529413354001\n",
      "    mean_processing_ms: 0.1350340114596774\n",
      "  time_since_restore: 189.98675966262817\n",
      "  time_this_iter_s: 1.2547879219055176\n",
      "  time_total_s: 189.98675966262817\n",
      "  timestamp: 1583835910\n",
      "  timesteps_since_restore: 264600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 264600\n",
      "  training_iteration: 150\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 189 s, 150 iter, 264600 ts, 9.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:10,458\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 264600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:11,654\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 266400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:12,823\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 268200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:14,006\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 270000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:15,167\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 271800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-16\n",
      "  done: false\n",
      "  episode_len_mean: 9.434554973821989\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 9.434554973821989\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 28584\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.983\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 766.665283203125\n",
      "        mean_q: 737.3280639648438\n",
      "        mean_td_error: 172.48509216308594\n",
      "        min_q: -51.594970703125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 273600\n",
      "    num_steps_trained: 363200\n",
      "    num_target_updates: 456\n",
      "    opt_peak_throughput: 13797.189\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.127\n",
      "    sample_time_ms: 259.421\n",
      "    update_time_ms: 5.001\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.35\n",
      "    ram_util_percent: 58.650000000000006\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0700746253053748\n",
      "    mean_inference_ms: 0.9073937147960914\n",
      "    mean_processing_ms: 0.1347236923306746\n",
      "  time_since_restore: 195.85045909881592\n",
      "  time_this_iter_s: 1.1719403266906738\n",
      "  time_total_s: 195.85045909881592\n",
      "  timestamp: 1583835916\n",
      "  timesteps_since_restore: 273600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 273600\n",
      "  training_iteration: 155\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 195 s, 155 iter, 273600 ts, 9.43 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:16,358\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 273600}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:17,530\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 275400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:18,692\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 277200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:19,856\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 279000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:21,022\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 280800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-22\n",
      "  done: false\n",
      "  episode_len_mean: 9.321243523316062\n",
      "  episode_reward_max: 11.0\n",
      "  episode_reward_mean: 9.321243523316062\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 29542\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.483\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 764.67333984375\n",
      "        mean_q: 732.8323974609375\n",
      "        mean_td_error: 193.17611694335938\n",
      "        min_q: -44.047027587890625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 282600\n",
      "    num_steps_trained: 375200\n",
      "    num_target_updates: 471\n",
      "    opt_peak_throughput: 14163.63\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.768\n",
      "    sample_time_ms: 258.829\n",
      "    update_time_ms: 5.304\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.900000000000006\n",
      "    ram_util_percent: 58.9\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06988881474931824\n",
      "    mean_inference_ms: 0.9046419576672489\n",
      "    mean_processing_ms: 0.134430209611449\n",
      "  time_since_restore: 201.6674075126648\n",
      "  time_this_iter_s: 1.170093297958374\n",
      "  time_total_s: 201.6674075126648\n",
      "  timestamp: 1583835922\n",
      "  timesteps_since_restore: 282600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 282600\n",
      "  training_iteration: 160\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 201 s, 160 iter, 282600 ts, 9.32 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:22,209\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 282600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:23,387\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 284400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:24,562\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 286200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:25,725\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 288000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:26,896\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 289800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-28\n",
      "  done: false\n",
      "  episode_len_mean: 9.476190476190476\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.476190476190476\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 30495\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.189\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 763.49658203125\n",
      "        mean_q: 749.47021484375\n",
      "        mean_td_error: 204.81069946289062\n",
      "        min_q: -35.978546142578125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 291600\n",
      "    num_steps_trained: 387200\n",
      "    num_target_updates: 486\n",
      "    opt_peak_throughput: 14237.683\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.97\n",
      "    sample_time_ms: 259.094\n",
      "    update_time_ms: 5.131\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.0\n",
      "    ram_util_percent: 59.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06974143658632045\n",
      "    mean_inference_ms: 0.9021043675809196\n",
      "    mean_processing_ms: 0.1341335939659922\n",
      "  time_since_restore: 207.50009989738464\n",
      "  time_this_iter_s: 1.162074089050293\n",
      "  time_total_s: 207.50009989738464\n",
      "  timestamp: 1583835928\n",
      "  timesteps_since_restore: 291600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 291600\n",
      "  training_iteration: 165\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 207 s, 165 iter, 291600 ts, 9.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:28,073\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 291600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:29,258\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 293400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:30,429\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 295200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:31,594\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 297000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:32,765\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 298800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-33\n",
      "  done: false\n",
      "  episode_len_mean: 9.364583333333334\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.364583333333334\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 31448\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.212\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 761.8355712890625\n",
      "        mean_q: 740.7372436523438\n",
      "        mean_td_error: 200.6641845703125\n",
      "        min_q: -27.713104248046875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 300600\n",
      "    num_steps_trained: 399200\n",
      "    num_target_updates: 501\n",
      "    opt_peak_throughput: 13983.128\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.366\n",
      "    sample_time_ms: 259.56\n",
      "    update_time_ms: 5.414\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.2\n",
      "    ram_util_percent: 59.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0695903361128592\n",
      "    mean_inference_ms: 0.8997819358027802\n",
      "    mean_processing_ms: 0.13386745062468386\n",
      "  time_since_restore: 213.34852981567383\n",
      "  time_this_iter_s: 1.172351598739624\n",
      "  time_total_s: 213.34852981567383\n",
      "  timestamp: 1583835933\n",
      "  timesteps_since_restore: 300600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 300600\n",
      "  training_iteration: 170\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 213 s, 170 iter, 300600 ts, 9.36 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:33,953\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 300600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:35,132\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 302400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:36,294\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 304200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:37,528\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 306000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:38,704\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 307800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-39\n",
      "  done: false\n",
      "  episode_len_mean: 9.34020618556701\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.34020618556701\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 194\n",
      "  episodes_total: 32402\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.226\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 757.427001953125\n",
      "        mean_q: 739.36181640625\n",
      "        mean_td_error: 209.6544952392578\n",
      "        min_q: -18.6856689453125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 309600\n",
      "    num_steps_trained: 411200\n",
      "    num_target_updates: 516\n",
      "    opt_peak_throughput: 14228.362\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.562\n",
      "    sample_time_ms: 267.372\n",
      "    update_time_ms: 5.15\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.4\n",
      "    ram_util_percent: 59.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06945631981617074\n",
      "    mean_inference_ms: 0.8981942349514593\n",
      "    mean_processing_ms: 0.1336739405631931\n",
      "  time_since_restore: 219.24963474273682\n",
      "  time_this_iter_s: 1.167705774307251\n",
      "  time_total_s: 219.24963474273682\n",
      "  timestamp: 1583835939\n",
      "  timesteps_since_restore: 309600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 309600\n",
      "  training_iteration: 175\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 219 s, 175 iter, 309600 ts, 9.34 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:39,887\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 309600}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:41,046\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 311400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:42,221\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 313200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:43,395\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 315000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:44,567\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 316800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-45\n",
      "  done: false\n",
      "  episode_len_mean: 9.465608465608465\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.465608465608465\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 189\n",
      "  episodes_total: 33352\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 59.595\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 754.9881591796875\n",
      "        mean_q: 731.6551513671875\n",
      "        mean_td_error: 187.2901611328125\n",
      "        min_q: -10.249786376953125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 318600\n",
      "    num_steps_trained: 423200\n",
      "    num_target_updates: 531\n",
      "    opt_peak_throughput: 13423.969\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.793\n",
      "    sample_time_ms: 260.933\n",
      "    update_time_ms: 5.392\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.150000000000006\n",
      "    ram_util_percent: 59.349999999999994\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06932985678921487\n",
      "    mean_inference_ms: 0.8960650420946862\n",
      "    mean_processing_ms: 0.13342548799184126\n",
      "  time_since_restore: 225.1285834312439\n",
      "  time_this_iter_s: 1.2167739868164062\n",
      "  time_total_s: 225.1285834312439\n",
      "  timestamp: 1583835945\n",
      "  timesteps_since_restore: 318600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 318600\n",
      "  training_iteration: 180\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 225 s, 180 iter, 318600 ts, 9.47 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:45,799\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 318600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:47,181\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 320400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:48,459\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 322200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:49,668\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 324000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-51\n",
      "  done: false\n",
      "  episode_len_mean: 9.447368421052632\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.447368421052632\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 34114\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 59.912\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 754.3896484375\n",
      "        mean_q: 732.8582763671875\n",
      "        mean_td_error: 186.56785583496094\n",
      "        min_q: -3.6619873046875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 325800\n",
      "    num_steps_trained: 432800\n",
      "    num_target_updates: 543\n",
      "    opt_peak_throughput: 13352.845\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 67.403\n",
      "    sample_time_ms: 288.852\n",
      "    update_time_ms: 5.088\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.1\n",
      "    ram_util_percent: 59.75\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06948103154748954\n",
      "    mean_inference_ms: 0.8974515025296887\n",
      "    mean_processing_ms: 0.1335904020669679\n",
      "  time_since_restore: 230.36009454727173\n",
      "  time_this_iter_s: 1.3754055500030518\n",
      "  time_total_s: 230.36009454727173\n",
      "  timestamp: 1583835951\n",
      "  timesteps_since_restore: 325800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 325800\n",
      "  training_iteration: 184\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 230 s, 184 iter, 325800 ts, 9.45 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:51,060\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 325800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:52,420\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 327000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:53,483\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 328200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:54,680\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 330000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:55,843\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 331800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-25-57\n",
      "  done: false\n",
      "  episode_len_mean: 9.478947368421053\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.478947368421053\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 34938\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.339\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 753.3751220703125\n",
      "        mean_q: 737.1546630859375\n",
      "        mean_td_error: 198.05413818359375\n",
      "        min_q: 3.634613037109375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 333600\n",
      "    num_steps_trained: 443200\n",
      "    num_target_updates: 556\n",
      "    opt_peak_throughput: 14199.634\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 64.912\n",
      "    sample_time_ms: 268.038\n",
      "    update_time_ms: 5.083\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.2\n",
      "    ram_util_percent: 59.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06968752719664391\n",
      "    mean_inference_ms: 0.9000047343877161\n",
      "    mean_processing_ms: 0.13388344726639126\n",
      "  time_since_restore: 236.26927590370178\n",
      "  time_this_iter_s: 1.1668734550476074\n",
      "  time_total_s: 236.26927590370178\n",
      "  timestamp: 1583835957\n",
      "  timesteps_since_restore: 333600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 333600\n",
      "  training_iteration: 189\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 236 s, 189 iter, 333600 ts, 9.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:57,018\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 333600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:58,186\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 335400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:25:59,382\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 337200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:00,554\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 339000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:01,725\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 340800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-02\n",
      "  done: false\n",
      "  episode_len_mean: 9.364583333333334\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.364583333333334\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 35891\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.097\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 752.358154296875\n",
      "        mean_q: 734.08251953125\n",
      "        mean_td_error: 185.54034423828125\n",
      "        min_q: 11.2578125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 342600\n",
      "    num_steps_trained: 455200\n",
      "    num_target_updates: 571\n",
      "    opt_peak_throughput: 14260.889\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.918\n",
      "    sample_time_ms: 260.551\n",
      "    update_time_ms: 5.523\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.9\n",
      "    ram_util_percent: 59.8\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06955441244486682\n",
      "    mean_inference_ms: 0.8980499216963681\n",
      "    mean_processing_ms: 0.1336361705963369\n",
      "  time_since_restore: 242.1029303073883\n",
      "  time_this_iter_s: 1.1506609916687012\n",
      "  time_total_s: 242.1029303073883\n",
      "  timestamp: 1583835962\n",
      "  timesteps_since_restore: 342600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 342600\n",
      "  training_iteration: 194\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 242 s, 194 iter, 342600 ts, 9.36 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:02,884\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 342600}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:04,066\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 344400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:05,246\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 346200}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,829\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-2.708, max=1.784, mean=-0.24)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,830\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,830\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-2.708, max=1.784, mean=-0.24)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,830\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-2.708, max=1.784, mean=-0.24)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,830\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-2.708, max=1.784, mean=-0.24),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,830\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,832\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=15.25, max=752.376, mean=383.813)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,833\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((10,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((10,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((10,), dtype=bool, min=0.0, max=1.0, mean=0.1),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((10,), dtype=int64, min=852559049.0, max=852559049.0, mean=852559049.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((10,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((10, 4), dtype=float32, min=-3.051, max=1.98, mean=-0.132),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((10, 4), dtype=float32, min=-2.708, max=1.784, mean=-0.103),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((10,), dtype=int64, min=0.0, max=1.0, mean=0.9),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((10,), dtype=float32, min=0.0, max=1.0, mean=0.9),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((10, 2), dtype=float32, min=15.205, max=752.376, mean=383.697),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((10,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((10,), dtype=int64, min=0.0, max=9.0, mean=4.5),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((10,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((10,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:05,860\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.995),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.105),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=62150096.0, max=1788771775.0, mean=933107716.98),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-3.299, max=2.195, mean=-0.142),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.951, max=2.0, mean=-0.112),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.89),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=15.123, max=752.376, mean=383.613),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=10.0, mean=4.355),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=578.0, max=578.0, mean=578.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:26:06,032\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:06,430\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 348000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,594\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 349800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,918\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,918\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,919\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:07,928\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 751.42395,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 731.9547,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': 196.76527,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': 17.7453,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-733.812, max=750.424, mean=196.765)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:08,300\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.974),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=3.0, max=49918.0, mean=32200.072),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.3),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-3.323, max=2.195, mean=-0.173),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.975, max=2.0, mean=-0.141),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=0.136, max=0.98, mean=0.159)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-08\n",
      "  done: false\n",
      "  episode_len_mean: 9.413612565445026\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.413612565445026\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 36847\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.007\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 750.6754150390625\n",
      "        mean_q: 733.0075073242188\n",
      "        mean_td_error: 197.8500213623047\n",
      "        min_q: 18.918853759765625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 351600\n",
      "    num_steps_trained: 467200\n",
      "    num_target_updates: 586\n",
      "    opt_peak_throughput: 14033.258\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.426\n",
      "    sample_time_ms: 259.811\n",
      "    update_time_ms: 5.319\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.9\n",
      "    ram_util_percent: 60.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06942913856677593\n",
      "    mean_inference_ms: 0.8962809722395011\n",
      "    mean_processing_ms: 0.1334183677008879\n",
      "  time_since_restore: 247.9467260837555\n",
      "  time_this_iter_s: 1.1568479537963867\n",
      "  time_total_s: 247.9467260837555\n",
      "  timestamp: 1583835968\n",
      "  timesteps_since_restore: 351600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 351600\n",
      "  training_iteration: 199\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 247 s, 199 iter, 351600 ts, 9.41 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:08,759\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 351600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:09,939\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 353400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:11,095\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 355200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:12,265\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 357000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:13,512\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 358800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-14\n",
      "  done: false\n",
      "  episode_len_mean: 9.51063829787234\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.51063829787234\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 188\n",
      "  episodes_total: 37801\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.969\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 746.3980102539062\n",
      "        mean_q: 724.5678100585938\n",
      "        mean_td_error: 189.34765625\n",
      "        min_q: 26.730926513671875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 360600\n",
      "    num_steps_trained: 479200\n",
      "    num_target_updates: 601\n",
      "    opt_peak_throughput: 13566.473\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 66.471\n",
      "    sample_time_ms: 260.968\n",
      "    update_time_ms: 5.136\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.9\n",
      "    ram_util_percent: 60.150000000000006\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.069318471429045\n",
      "    mean_inference_ms: 0.8944504915631065\n",
      "    mean_processing_ms: 0.1332429615755904\n",
      "  time_since_restore: 253.8383321762085\n",
      "  time_this_iter_s: 1.1611735820770264\n",
      "  time_total_s: 253.8383321762085\n",
      "  timestamp: 1583835974\n",
      "  timesteps_since_restore: 360600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 360600\n",
      "  training_iteration: 204\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 253 s, 204 iter, 360600 ts, 9.51 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:14,681\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 360600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:15,860\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 362400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:17,033\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 364200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:18,188\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 366000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:19,373\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 367800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-20\n",
      "  done: false\n",
      "  episode_len_mean: 9.478947368421053\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.478947368421053\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 38755\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.596\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 744.7872314453125\n",
      "        mean_q: 729.4729614257812\n",
      "        mean_td_error: 202.50465393066406\n",
      "        min_q: 34.625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 369600\n",
      "    num_steps_trained: 491200\n",
      "    num_target_updates: 616\n",
      "    opt_peak_throughput: 14135.294\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.92\n",
      "    sample_time_ms: 259.325\n",
      "    update_time_ms: 5.152\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.45\n",
      "    ram_util_percent: 60.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06921502932848979\n",
      "    mean_inference_ms: 0.8925971302189896\n",
      "    mean_processing_ms: 0.13304418825243072\n",
      "  time_since_restore: 259.661563873291\n",
      "  time_this_iter_s: 1.1564395427703857\n",
      "  time_total_s: 259.661563873291\n",
      "  timestamp: 1583835980\n",
      "  timesteps_since_restore: 369600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 369600\n",
      "  training_iteration: 209\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 259 s, 209 iter, 369600 ts, 9.48 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:20,538\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 369600}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:21,724\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 371400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:22,896\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 373200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:24,064\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 375000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:25,232\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 376800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 9.393782383419689\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.393782383419689\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 39708\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.678\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 742.567626953125\n",
      "        mean_q: 725.427978515625\n",
      "        mean_td_error: 212.8330535888672\n",
      "        min_q: 42.162506103515625\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 378600\n",
      "    num_steps_trained: 503200\n",
      "    num_target_updates: 631\n",
      "    opt_peak_throughput: 14368.317\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.235\n",
      "    sample_time_ms: 271.218\n",
      "    update_time_ms: 5.07\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.6\n",
      "    ram_util_percent: 60.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06913160961122944\n",
      "    mean_inference_ms: 0.8916943560613053\n",
      "    mean_processing_ms: 0.13293767401467707\n",
      "  time_since_restore: 265.6222369670868\n",
      "  time_this_iter_s: 1.2888739109039307\n",
      "  time_total_s: 265.6222369670868\n",
      "  timestamp: 1583835986\n",
      "  timesteps_since_restore: 378600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 378600\n",
      "  training_iteration: 214\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 265 s, 214 iter, 378600 ts, 9.39 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:26,529\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 378600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:27,700\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 380400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:28,874\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 382200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:30,033\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 384000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:31,198\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 385800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-32\n",
      "  done: false\n",
      "  episode_len_mean: 9.44502617801047\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.44502617801047\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 40662\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.117\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 740.0535278320312\n",
      "        mean_q: 725.107177734375\n",
      "        mean_td_error: 205.9874267578125\n",
      "        min_q: 49.604583740234375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 387600\n",
      "    num_steps_trained: 515200\n",
      "    num_target_updates: 646\n",
      "    opt_peak_throughput: 14255.83\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 63.072\n",
      "    sample_time_ms: 260.229\n",
      "    update_time_ms: 4.855\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.85\n",
      "    ram_util_percent: 60.4\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06903257067836603\n",
      "    mean_inference_ms: 0.8901443037737365\n",
      "    mean_processing_ms: 0.1327414864176743\n",
      "  time_since_restore: 271.43873620033264\n",
      "  time_this_iter_s: 1.1700937747955322\n",
      "  time_total_s: 271.43873620033264\n",
      "  timestamp: 1583835992\n",
      "  timesteps_since_restore: 387600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 387600\n",
      "  training_iteration: 219\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 271 s, 219 iter, 387600 ts, 9.45 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:32,375\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 387600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:33,578\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 389400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:35,096\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 391200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:36,450\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 393000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-37\n",
      "  done: false\n",
      "  episode_len_mean: 9.46808510638298\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.46808510638298\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 188\n",
      "  episodes_total: 41425\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.386\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 738.3040771484375\n",
      "        mean_q: 720.9232788085938\n",
      "        mean_td_error: 190.09359741210938\n",
      "        min_q: 55.25067138671875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 394800\n",
      "    num_steps_trained: 524800\n",
      "    num_target_updates: 658\n",
      "    opt_peak_throughput: 14187.974\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.517\n",
      "    sample_time_ms: 309.234\n",
      "    update_time_ms: 5.268\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.5\n",
      "    ram_util_percent: 60.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06912776749697573\n",
      "    mean_inference_ms: 0.8915468768563867\n",
      "    mean_processing_ms: 0.1330000140886569\n",
      "  time_since_restore: 276.6473226547241\n",
      "  time_this_iter_s: 1.1528103351593018\n",
      "  time_total_s: 276.6473226547241\n",
      "  timestamp: 1583835997\n",
      "  timesteps_since_restore: 394800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 394800\n",
      "  training_iteration: 223\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 276 s, 223 iter, 394800 ts, 9.47 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:37,612\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 394800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:38,784\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 396600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:40,038\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 397800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:41,136\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 399000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:42,166\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 400200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 9.553191489361701\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.553191489361701\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 188\n",
      "  episodes_total: 42188\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 67.11\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 737.8740234375\n",
      "        mean_q: 721.6113891601562\n",
      "        mean_td_error: 207.39723205566406\n",
      "        min_q: 61.105316162109375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 534400\n",
      "    num_target_updates: 670\n",
      "    opt_peak_throughput: 11920.733\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 72.458\n",
      "    sample_time_ms: 355.918\n",
      "    update_time_ms: 5.48\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.2\n",
      "    ram_util_percent: 61.349999999999994\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06933150141238774\n",
      "    mean_inference_ms: 0.8951779209317567\n",
      "    mean_processing_ms: 0.13349913073482922\n",
      "  time_since_restore: 282.4802141189575\n",
      "  time_this_iter_s: 1.3101766109466553\n",
      "  time_total_s: 282.4802141189575\n",
      "  timestamp: 1583836003\n",
      "  timesteps_since_restore: 402000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 228\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 282 s, 228 iter, 402000 ts, 9.55 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:43,484\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 402000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:44,658\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 403800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:45,867\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 405600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:47,408\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 407400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-48\n",
      "  done: false\n",
      "  episode_len_mean: 9.536842105263158\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.536842105263158\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 42947\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 64.339\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 737.6187133789062\n",
      "        mean_q: 721.4282836914062\n",
      "        mean_td_error: 182.7646942138672\n",
      "        min_q: 66.42987060546875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 409200\n",
      "    num_steps_trained: 544000\n",
      "    num_target_updates: 682\n",
      "    opt_peak_throughput: 12434.107\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 70.045\n",
      "    sample_time_ms: 291.367\n",
      "    update_time_ms: 5.497\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.69999999999999\n",
      "    ram_util_percent: 61.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06933304566059902\n",
      "    mean_inference_ms: 0.8952845985773069\n",
      "    mean_processing_ms: 0.13351400065522948\n",
      "  time_since_restore: 287.61774492263794\n",
      "  time_this_iter_s: 1.2357676029205322\n",
      "  time_total_s: 287.61774492263794\n",
      "  timestamp: 1583836008\n",
      "  timesteps_since_restore: 409200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 409200\n",
      "  training_iteration: 232\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 287 s, 232 iter, 409200 ts, 9.54 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:48,652\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 409200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:49,930\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 411000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:51,046\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 412200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:52,354\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 414000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-53\n",
      "  done: false\n",
      "  episode_len_mean: 9.465968586387435\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.465968586387435\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 43645\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.655\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 737.7658081054688\n",
      "        mean_q: 723.28466796875\n",
      "        mean_td_error: 197.33563232421875\n",
      "        min_q: 71.6522216796875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 415800\n",
      "    num_steps_trained: 552800\n",
      "    num_target_updates: 693\n",
      "    opt_peak_throughput: 12768.238\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 72.098\n",
      "    sample_time_ms: 322.258\n",
      "    update_time_ms: 6.344\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.9\n",
      "    ram_util_percent: 61.1\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06945775867966218\n",
      "    mean_inference_ms: 0.8973387642451943\n",
      "    mean_processing_ms: 0.13374268064444886\n",
      "  time_since_restore: 292.7448718547821\n",
      "  time_this_iter_s: 1.4469523429870605\n",
      "  time_total_s: 292.7448718547821\n",
      "  timestamp: 1583836013\n",
      "  timesteps_since_restore: 415800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 415800\n",
      "  training_iteration: 236\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 292 s, 236 iter, 415800 ts, 9.47 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:53,809\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 415800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:54,976\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 417600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:56,136\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 419400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:57,387\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 421200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:58,566\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 423000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-26-59\n",
      "  done: false\n",
      "  episode_len_mean: 9.447916666666666\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.447916666666666\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 192\n",
      "  episodes_total: 44606\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 56.514\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 738.5852661132812\n",
      "        mean_q: 716.131591796875\n",
      "        mean_td_error: 191.43896484375\n",
      "        min_q: 79.92022705078125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 424800\n",
      "    num_steps_trained: 564800\n",
      "    num_target_updates: 708\n",
      "    opt_peak_throughput: 14155.76\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.834\n",
      "    sample_time_ms: 264.65\n",
      "    update_time_ms: 5.074\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.25\n",
      "    ram_util_percent: 61.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06936945098621854\n",
      "    mean_inference_ms: 0.8957584837903102\n",
      "    mean_processing_ms: 0.13356101120795086\n",
      "  time_since_restore: 298.6369662284851\n",
      "  time_this_iter_s: 1.1615889072418213\n",
      "  time_total_s: 298.6369662284851\n",
      "  timestamp: 1583836019\n",
      "  timesteps_since_restore: 424800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 424800\n",
      "  training_iteration: 241\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 298 s, 241 iter, 424800 ts, 9.45 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:26:59,736\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 424800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:00,914\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 426600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:02,071\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 428400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:03,227\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 430200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:04,399\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 432000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-05\n",
      "  done: false\n",
      "  episode_len_mean: 9.574468085106384\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.574468085106384\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 188\n",
      "  episodes_total: 45550\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.627\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 735.6724243164062\n",
      "        mean_q: 719.1492309570312\n",
      "        mean_td_error: 157.91114807128906\n",
      "        min_q: 88.64218139648438\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 433800\n",
      "    num_steps_trained: 576800\n",
      "    num_target_updates: 723\n",
      "    opt_peak_throughput: 13645.532\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.181\n",
      "    sample_time_ms: 264.72\n",
      "    update_time_ms: 5.494\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.1\n",
      "    ram_util_percent: 61.349999999999994\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0692742877323931\n",
      "    mean_inference_ms: 0.894419789464434\n",
      "    mean_processing_ms: 0.13338458069695355\n",
      "  time_since_restore: 304.5580406188965\n",
      "  time_this_iter_s: 1.2831134796142578\n",
      "  time_total_s: 304.5580406188965\n",
      "  timestamp: 1583836025\n",
      "  timesteps_since_restore: 433800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 433800\n",
      "  training_iteration: 246\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 304 s, 246 iter, 433800 ts, 9.57 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:05,690\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 433800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,092\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-1.804, max=1.182, mean=-0.176),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,093\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,094\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=89.959, max=735.44, mean=412.699)})}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,094\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-2.126, max=1.378, mean=-0.211)}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,094\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,094\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-2.126, max=1.378, mean=-0.211)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,094\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-2.126, max=1.378, mean=-0.211)\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,098\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((3,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'agent_index': np.ndarray((3,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'dones': np.ndarray((3,), dtype=bool, min=0.0, max=1.0, mean=0.333),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'eps_id': np.ndarray((3,), dtype=int64, min=1646145983.0, max=1646145983.0, mean=1646145983.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'infos': np.ndarray((3,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'new_obs': np.ndarray((3, 4), dtype=float32, min=-2.799, max=1.77, mean=-0.249),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'obs': np.ndarray((3, 4), dtype=float32, min=-2.457, max=1.574, mean=-0.212),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_actions': np.ndarray((3,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'prev_rewards': np.ndarray((3,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'q_values': np.ndarray((3, 2), dtype=float32, min=89.959, max=735.44, mean=412.699),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'rewards': np.ndarray((3,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         't': np.ndarray((3,), dtype=int64, min=6.0, max=8.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'unroll_id': np.ndarray((3,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m                         'weights': np.ndarray((3,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,311\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.98),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.105),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=8689434.0, max=1943110823.0, mean=1058173964.98),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-3.092, max=1.995, mean=-0.146),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.781, max=1.799, mean=-0.116),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.885),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.895),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=89.565, max=735.44, mean=412.588),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=11.0, mean=4.275),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=724.0, max=724.0, mean=724.0),\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6483)\u001b[0m 2020-03-10 11:27:06,484\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:06,891\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 435600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,067\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 437400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,403\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,403\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(100, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/kernel:0' shape=(4, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(100, 100) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,404\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,413\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'max_q': 734.8357,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_q': 723.0433,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'mean_td_error': 195.42955,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'min_q': 92.77832,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                       'td_error': np.ndarray((800,), dtype=float32, min=-642.133, max=733.836, mean=195.43)}}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:08,796\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m { 'count': 800,\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((800,), dtype=int64, min=0.0, max=1.0, mean=0.981),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'batch_indexes': np.ndarray((800,), dtype=int64, min=4.0, max=49966.0, mean=27365.194),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'dones': np.ndarray((800,), dtype=bool, min=0.0, max=1.0, mean=0.287),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'new_obs': np.ndarray((800, 4), dtype=float32, min=-3.308, max=2.18, mean=-0.167),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'obs': np.ndarray((800, 4), dtype=float32, min=-2.96, max=1.984, mean=-0.135),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'rewards': np.ndarray((800,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                                     'weights': np.ndarray((800,), dtype=float64, min=0.142, max=0.999, mean=0.167)},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:09,242\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 439200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:10,403\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 441000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-11\n",
      "  done: false\n",
      "  episode_len_mean: 9.403141361256544\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.403141361256544\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 46502\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 55.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 734.89208984375\n",
      "        mean_q: 715.3982543945312\n",
      "        mean_td_error: 194.21189880371094\n",
      "        min_q: 97.38812255859375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 442800\n",
      "    num_steps_trained: 588800\n",
      "    num_target_updates: 738\n",
      "    opt_peak_throughput: 14287.668\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 62.232\n",
      "    sample_time_ms: 261.535\n",
      "    update_time_ms: 5.13\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.2\n",
      "    ram_util_percent: 61.6\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06917946686217948\n",
      "    mean_inference_ms: 0.8930131386143755\n",
      "    mean_processing_ms: 0.1332276193613361\n",
      "  time_since_restore: 310.4079658985138\n",
      "  time_this_iter_s: 1.158780813217163\n",
      "  time_total_s: 310.4079658985138\n",
      "  timestamp: 1583836031\n",
      "  timesteps_since_restore: 442800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 442800\n",
      "  training_iteration: 251\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 310 s, 251 iter, 442800 ts, 9.4 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:11,571\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 442800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:12,753\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 444600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:14,015\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 446400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:15,187\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 448200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:16,478\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 450000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-17\n",
      "  done: false\n",
      "  episode_len_mean: 9.614973262032086\n",
      "  episode_reward_max: 14.0\n",
      "  episode_reward_mean: 9.614973262032086\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 187\n",
      "  episodes_total: 47452\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.058\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 735.00927734375\n",
      "        mean_q: 723.0923461914062\n",
      "        mean_td_error: 198.16957092285156\n",
      "        min_q: 105.99920654296875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 451800\n",
      "    num_steps_trained: 600800\n",
      "    num_target_updates: 753\n",
      "    opt_peak_throughput: 13779.251\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.689\n",
      "    sample_time_ms: 279.732\n",
      "    update_time_ms: 5.477\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.849999999999994\n",
      "    ram_util_percent: 61.7\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0691748046353341\n",
      "    mean_inference_ms: 0.8928210613727631\n",
      "    mean_processing_ms: 0.1331868104112221\n",
      "  time_since_restore: 316.5821454524994\n",
      "  time_this_iter_s: 1.2910678386688232\n",
      "  time_total_s: 316.5821454524994\n",
      "  timestamp: 1583836037\n",
      "  timesteps_since_restore: 451800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 451800\n",
      "  training_iteration: 256\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 316 s, 256 iter, 451800 ts, 9.61 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:17,777\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 451800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:18,967\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 453600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:20,145\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 455400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:21,310\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 457200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:22,480\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 459000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-23\n",
      "  done: false\n",
      "  episode_len_mean: 9.305699481865284\n",
      "  episode_reward_max: 12.0\n",
      "  episode_reward_mean: 9.305699481865284\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 193\n",
      "  episodes_total: 48407\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 57.869\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 734.7434692382812\n",
      "        mean_q: 719.861572265625\n",
      "        mean_td_error: 193.3973388671875\n",
      "        min_q: 114.52407836914062\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 460800\n",
      "    num_steps_trained: 612800\n",
      "    num_target_updates: 768\n",
      "    opt_peak_throughput: 13824.212\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 65.682\n",
      "    sample_time_ms: 276.053\n",
      "    update_time_ms: 5.017\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.0\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06910819025360816\n",
      "    mean_inference_ms: 0.8919176267585596\n",
      "    mean_processing_ms: 0.1331036225315804\n",
      "  time_since_restore: 322.63362979888916\n",
      "  time_this_iter_s: 1.372528314590454\n",
      "  time_total_s: 322.63362979888916\n",
      "  timestamp: 1583836043\n",
      "  timesteps_since_restore: 460800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 460800\n",
      "  training_iteration: 261\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 322 s, 261 iter, 460800 ts, 9.31 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:23,860\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 460800}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:25,214\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 462600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:26,510\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 464400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:27,536\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 465600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:28,594\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 466800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-29\n",
      "  done: false\n",
      "  episode_len_mean: 9.505263157894737\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.505263157894737\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 49229\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 61.012\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 735.6856079101562\n",
      "        mean_q: 723.22021484375\n",
      "        mean_td_error: 219.9229278564453\n",
      "        min_q: 122.31634521484375\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 468600\n",
      "    num_steps_trained: 623200\n",
      "    num_target_updates: 781\n",
      "    opt_peak_throughput: 13112.122\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 69.788\n",
      "    sample_time_ms: 323.817\n",
      "    update_time_ms: 5.231\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.8\n",
      "    ram_util_percent: 62.05\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06924146451357523\n",
      "    mean_inference_ms: 0.8938524623204288\n",
      "    mean_processing_ms: 0.1333298193290878\n",
      "  time_since_restore: 328.62536787986755\n",
      "  time_this_iter_s: 1.2845215797424316\n",
      "  time_total_s: 328.62536787986755\n",
      "  timestamp: 1583836049\n",
      "  timesteps_since_restore: 468600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 468600\n",
      "  training_iteration: 266\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 328 s, 266 iter, 468600 ts, 9.51 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:29,889\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 468600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:31,163\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 470400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:32,346\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 472200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:33,500\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 474000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:34,798\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 475800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-36\n",
      "  done: false\n",
      "  episode_len_mean: 9.460732984293193\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.460732984293193\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 191\n",
      "  episodes_total: 50182\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 58.991\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 735.908935546875\n",
      "        mean_q: 725.2283325195312\n",
      "        mean_td_error: 213.85992431640625\n",
      "        min_q: 132.04119873046875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 477600\n",
      "    num_steps_trained: 635200\n",
      "    num_target_updates: 796\n",
      "    opt_peak_throughput: 13561.346\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 66.745\n",
      "    sample_time_ms: 278.408\n",
      "    update_time_ms: 4.727\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.95\n",
      "    ram_util_percent: 62.0\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06920647542948193\n",
      "    mean_inference_ms: 0.8933680392619355\n",
      "    mean_processing_ms: 0.13327612058754834\n",
      "  time_since_restore: 334.8177840709686\n",
      "  time_this_iter_s: 1.3057024478912354\n",
      "  time_total_s: 334.8177840709686\n",
      "  timestamp: 1583836056\n",
      "  timesteps_since_restore: 477600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 477600\n",
      "  training_iteration: 271\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 334 s, 271 iter, 477600 ts, 9.46 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:36,114\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 477600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:37,528\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 479400}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:38,936\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 481200}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:40,396\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 483000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-27-41\n",
      "  done: false\n",
      "  episode_len_mean: 9.463157894736842\n",
      "  episode_reward_max: 13.0\n",
      "  episode_reward_mean: 9.463157894736842\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 190\n",
      "  episodes_total: 50945\n",
      "  experiment_id: 95da8384be1d4804b585e4823a589caf\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 62.003\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        max_q: 734.766845703125\n",
      "        mean_q: 712.3275146484375\n",
      "        mean_td_error: 186.5863037109375\n",
      "        min_q: 139.26922607421875\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 484800\n",
      "    num_steps_trained: 644800\n",
      "    num_target_updates: 808\n",
      "    opt_peak_throughput: 12902.608\n",
      "    opt_samples: 800.0\n",
      "    replay_time_ms: 68.0\n",
      "    sample_time_ms: 305.114\n",
      "    update_time_ms: 5.689\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.0\n",
      "    ram_util_percent: 62.3\n",
      "  pid: 6484\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06930519224596494\n",
      "    mean_inference_ms: 0.8947450165437812\n",
      "    mean_processing_ms: 0.13349298033347426\n",
      "  time_since_restore: 340.24549102783203\n",
      "  time_this_iter_s: 1.1617083549499512\n",
      "  time_total_s: 340.24549102783203\n",
      "  timestamp: 1583836061\n",
      "  timesteps_since_restore: 484800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 484800\n",
      "  training_iteration: 275\n",
      "  trial_id: f79c2a12\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6484], 340 s, 275 iter, 484800 ts, 9.46 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:41,576\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 484800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:42,608\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 486000}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:43,791\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 487800}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:45,108\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 489600}\n",
      "\u001b[2m\u001b[36m(pid=6484)\u001b[0m 2020-03-10 11:27:46,377\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 491400}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 11:27:47,455\tERROR worker.py:1616 -- print_logs: Error 111 connecting to 192.168.2.105:39319. Connection refused.\n",
      "2020-03-10 11:27:47,456\tERROR import_thread.py:89 -- ImportThread: Error 111 connecting to 192.168.2.105:39319. Connection refused.\n",
      "2020-03-10 11:27:47,458\tERROR worker.py:1716 -- listen_error_messages_raylet: Error 111 connecting to 192.168.2.105:39319. Connection refused.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-cc0e7eb6843e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mtrial_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             return_trials=True)\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mlast_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_debug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mDEBUG_PRINT_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   2370\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m             \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m         )\n\u001b[1;32m   2374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.RayletClient.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/exceptions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_exc)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = run_experiments(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
