{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the TestEnv environment is used to simply simulate the network\n",
    "from flow.envs import TestEnv\n",
    "\n",
    "# the Experiment class is used for running simulations\n",
    "from flow.core.experiment import Experiment\n",
    "\n",
    "# the base network class\n",
    "from flow.networks import Network\n",
    "from flow.envs import Env\n",
    "\n",
    "# all other imports are standard\n",
    "from flow.core.params import VehicleParams\n",
    "from flow.core.params import NetParams\n",
    "from flow.core.params import InitialConfig\n",
    "from flow.core.params import EnvParams\n",
    "from flow.core.params import TrafficLightParams\n",
    "from flow.controllers import IDMController\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "\n",
    "# create some default parameters parameters\n",
    "HORIZON=2000\n",
    "env_params = EnvParams(horizon=HORIZON)\n",
    "initial_config = InitialConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dir = \"/home/valentin/Schreibtisch/personal_sumo_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(render=True, sim_step=1, restart_instance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles=VehicleParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InFlows\n",
    "\n",
    "inflow = InFlows()\n",
    "\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_east\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_south\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_north\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_north\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_south\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_west\",\n",
    "           probability=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flow_0',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_east',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_1',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_south',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_2',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_north',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_3',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_north',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_4',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_south',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_5',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_west',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inflow.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "net_params = NetParams(\n",
    "    inflows=inflow,\n",
    "    template={\n",
    "        # network geometry features\n",
    "        \"net\": os.path.join(le_dir, \"lemgo_small.net.xml\"),\n",
    "        # features associated with the properties of drivers\n",
    "        \"vtype\": os.path.join(le_dir, \"vtypes.add.xml\"),\n",
    "        # features associated with the routes vehicles take\n",
    "        \"rou\": os.path.join(le_dir, \"lemgo_small2_out.rou.xml\"),\n",
    "        \"det\": os.path.join(le_dir, \"lemgo_small.add.xml\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom network with lane area detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Running the Modified Simulation\n",
    "\n",
    "Finally, the fully imported simulation can be run as follows. \n",
    "\n",
    "**Warning**: the network takes time to initialize while the departure positions and times and vehicles are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FatalTraCIError",
     "evalue": "connection closed by SUMO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-23de0ae0028f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# run the simulation for 100000 steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/flow/flow/core/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_runs, num_steps, rl_actions, convert_to_csv)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 vel[j] = np.mean(\n\u001b[1;32m    122\u001b[0m                     self.env.k.vehicle.get_speed(self.env.k.vehicle.get_ids()))\n",
      "\u001b[0;32m~/flow/flow/envs/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, rl_actions)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0;31m# advance the simulation in the simulator by one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;31m# store new observations in the vehicles and traffic lights class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/flow/flow/core/kernel/simulation/traci.py\u001b[0m in \u001b[0;36msimulation_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"\"\"See parent class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubscriptionResults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0msubscriptionResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connection closed by SUMO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFatalTraCIError\u001b[0m: connection closed by SUMO"
     ]
    }
   ],
   "source": [
    "# create the network\n",
    "network = Network(\n",
    "    name=\"template\",\n",
    "    net_params=net_params,\n",
    "    vehicles=vehicles\n",
    ")\n",
    "\n",
    "# create the environment\n",
    "env = TestEnv(\n",
    "    env_params=env_params,\n",
    "    sim_params=sim_params,\n",
    "    network=network\n",
    ")\n",
    "\n",
    "# run the simulation for 100000 steps\n",
    "exp = Experiment(env=env)\n",
    "_ = exp.run(1, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the custom environment\n",
    "# Needs to be important in order to work properly in flow\n",
    "from flow.envs.simple_env import SimpleEnv\n",
    "env_name = SimpleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=\"first_exp\",\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=Network,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=sim_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=VehicleParams(),\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments, run\n",
    "from ray.tune.experiment import Experiment\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 11:52:48,261\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-01_11-52-48_258796_6450/logs.\n",
      "2020-04-01 11:52:48,421\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:47989 to respond...\n",
      "2020-04-01 11:52:48,561\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:45469 to respond...\n",
      "2020-04-01 11:52:48,574\tINFO services.py:809 -- Starting Redis shard with 1.65 GB max memory.\n",
      "2020-04-01 11:52:48,622\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-01_11-52-48_258796_6450/logs.\n",
      "2020-04-01 11:52:48,626\tINFO services.py:1475 -- Starting the Plasma object store with 2.47 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.105',\n",
       " 'redis_address': '192.168.2.105:47989',\n",
       " 'object_store_address': '/tmp/ray/session_2020-04-01_11-52-48_258796_6450/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-04-01_11-52-48_258796_6450/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-04-01_11-52-48_258796_6450'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(num_cpus=N_CPUS)#, object_store_memory=1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(config):\n",
    "    # ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
    "    # ensure we run at least one sgd iter\n",
    "    if config[\"num_sgd_iter\"] < 1:\n",
    "        config[\"num_sgd_iter\"] = 1\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=4,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            \"vf_clip_param\": lambda: random.uniform(20000, 50000),\n",
    "            \"lr\": [5e-2, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(N_CPUS*HORIZON, 2*N_CPUS*HORIZON),\n",
    "        },\n",
    "        custom_explore_fn=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "# #      \"class registered in the tune registry.\")\n",
    "alg_run = \"DQN\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = 0  # number of parallel workers\n",
    "# config[\"num_envs_per_worker\"] = 1  # number of parallel workers\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"lr\"] = 1e-3\n",
    "# config[\"v_max\"] = 0\n",
    "# config[\"v_min\"] = -50000\n",
    "config[\"train_batch_size\"] = 128  # batch size\n",
    "config[\"sample_batch_size\"] = 16  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [128]})  # size of hidden layers in network\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "config[\"timesteps_per_iteration\"] = HORIZON  \n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "config[\"env\"] = gym_name\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(flow_params[\"exp_tag\"], **{\n",
    "        \"run\": alg_run,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 5,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 5,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 100,  # 222number of iterations to stop after\n",
    "        },\n",
    "        \"num_samples\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 11:52:49,345\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-04-01 11:52:49,369\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-04-01 11:52:49,381\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-04-01 11:52:49,384\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.1/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:51,602\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:52.669803: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:52.710664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:52.711099: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5a94990 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:52.711123: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/fcnet_v1.py:48: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:77: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m 2020-04-01 11:52:56,194\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-07\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22579.0\n",
      "  episode_reward_mean: -22579.0\n",
      "  episode_reward_min: -22579.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 435.138\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.386033535003662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001857918978203088\n",
      "        policy_loss: -0.0006841583526693285\n",
      "        total_loss: 430364.40625\n",
      "        vf_explained_var: 1.9371509552001953e-05\n",
      "        vf_loss: 430364.46875\n",
      "    load_time_ms: 105.759\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 1000\n",
      "    sample_time_ms: 10876.767\n",
      "    update_time_ms: 0.009\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.80588235294118\n",
      "    ram_util_percent: 39.864705882352936\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.737345437308054\n",
      "    mean_inference_ms: 0.9141770990697535\n",
      "    mean_processing_ms: 1.205830664544196\n",
      "  time_since_restore: 11.470765829086304\n",
      "  time_this_iter_s: 11.470765829086304\n",
      "  time_total_s: 11.470765829086304\n",
      "  timestamp: 1585734787\n",
      "  timesteps_since_restore: 1000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 8bdc7078\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.3/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 11 s, 1 iter, 1000 ts, -2.26e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.65'; collision with vehicle 'flow_20.71', lane=':right_junction_8_0', gap=-1.00, time=735.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.65' ends teleporting on edge '-gneE28', time 735.00.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.68'; collision with vehicle 'flow_20.76', lane=':right_junction_8_0', gap=-1.00, time=761.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.68' ends teleporting on edge '-gneE28', time 761.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-17\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22579.0\n",
      "  episode_reward_mean: -22861.5\n",
      "  episode_reward_min: -23144.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 243.54\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3858383893966675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010224748257314786\n",
      "        policy_loss: -0.0006257534259930253\n",
      "        total_loss: 452419.0\n",
      "        vf_explained_var: 3.451108932495117e-05\n",
      "        vf_loss: 452418.9375\n",
      "    load_time_ms: 53.774\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "    sample_time_ms: 10297.276\n",
      "    update_time_ms: 0.006\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.514285714285716\n",
      "    ram_util_percent: 40.10000000000001\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.489228910418221\n",
      "    mean_inference_ms: 0.8771395494636843\n",
      "    mean_processing_ms: 1.2046331277208397\n",
      "  time_since_restore: 21.24328899383545\n",
      "  time_this_iter_s: 9.772523164749146\n",
      "  time_total_s: 21.24328899383545\n",
      "  timestamp: 1585734797\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 8bdc7078\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.3/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 21 s, 2 iter, 2000 ts, -2.29e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.23'; collision with vehicle 'flow_20.28', lane=':right_junction_8_0', gap=-1.00, time=332.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.23' ends teleporting on edge '-gneE28', time 332.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.78'; collision with vehicle 'flow_20.85', lane=':right_junction_8_0', gap=-1.00, time=892.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.78' ends teleporting on edge '-gneE28', time 892.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22579.0\n",
      "  episode_reward_mean: -22793.666666666668\n",
      "  episode_reward_min: -23144.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 179.413\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05000000074505806\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3858782052993774\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0001269898930331692\n",
      "        policy_loss: -0.0011236558202654123\n",
      "        total_loss: 434661.75\n",
      "        vf_explained_var: 7.43865966796875e-05\n",
      "        vf_loss: 434661.75\n",
      "    load_time_ms: 36.412\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 3000\n",
      "    sample_time_ms: 10170.345\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.692857142857143\n",
      "    ram_util_percent: 39.521428571428565\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.370674345498044\n",
      "    mean_inference_ms: 0.8586581570587097\n",
      "    mean_processing_ms: 1.204576899919391\n",
      "  time_since_restore: 31.213698863983154\n",
      "  time_this_iter_s: 9.970409870147705\n",
      "  time_total_s: 31.213698863983154\n",
      "  timestamp: 1585734807\n",
      "  timesteps_since_restore: 3000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 8bdc7078\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.3/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 31 s, 3 iter, 3000 ts, -2.28e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.55'; collision with vehicle 'flow_20.60', lane=':right_junction_8_0', gap=-1.00, time=704.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.55' ends teleporting on edge '-gneE28', time 704.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-37\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22200.0\n",
      "  episode_reward_mean: -22645.25\n",
      "  episode_reward_min: -23144.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 4\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 149.287\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.02500000037252903\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.386102318763733\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00010431060218252242\n",
      "        policy_loss: -0.0003530549874994904\n",
      "        total_loss: 418730.1875\n",
      "        vf_explained_var: 0.00010585784912109375\n",
      "        vf_loss: 418730.1875\n",
      "    load_time_ms: 27.763\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    sample_time_ms: 10096.917\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.192857142857136\n",
      "    ram_util_percent: 39.91428571428572\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.294974737704083\n",
      "    mean_inference_ms: 0.8488490314764316\n",
      "    mean_processing_ms: 1.20341331773451\n",
      "  time_since_restore: 41.15218257904053\n",
      "  time_this_iter_s: 9.938483715057373\n",
      "  time_total_s: 41.15218257904053\n",
      "  timestamp: 1585734817\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 8bdc7078\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.3/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 41 s, 4 iter, 4000 ts, -2.26e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.55'; collision with vehicle 'flow_20.57', lane=':right_junction_8_0', gap=-1.00, time=616.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.55' ends teleporting on edge '-gneE28', time 616.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.69'; collision with vehicle 'flow_50.76', lane=':left_junction_2_0', gap=-1.00, time=791.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.69' ends teleporting on edge 'gneE21', time 791.00.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-47\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22200.0\n",
      "  episode_reward_mean: -22773.4\n",
      "  episode_reward_min: -23286.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 5\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 129.452\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.012500000186264515\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3859877586364746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 6.132602720754221e-05\n",
      "        policy_loss: -0.0005418739165179431\n",
      "        total_loss: 458858.0625\n",
      "        vf_explained_var: 0.0001277327537536621\n",
      "        vf_loss: 458858.0625\n",
      "    load_time_ms: 22.549\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 5000\n",
      "    sample_time_ms: 10169.009\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.093333333333334\n",
      "    ram_util_percent: 40.666666666666664\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.26411280651959\n",
      "    mean_inference_ms: 0.8424903372092281\n",
      "    mean_processing_ms: 1.2031717246038958\n",
      "  time_since_restore: 51.66243028640747\n",
      "  time_this_iter_s: 10.510247707366943\n",
      "  time_total_s: 51.66243028640747\n",
      "  timestamp: 1585734827\n",
      "  timesteps_since_restore: 5000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 8bdc7078\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 11:53:48,734\tWARNING util.py:145 -- The `save_to_disk` operation took 0.7790172100067139 seconds to complete, which may be a performance bottleneck.\n",
      "2020-04-01 11:53:48,738\tWARNING util.py:145 -- The `process_trial` operation took 0.7880444526672363 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.4/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 51 s, 5 iter, 5000 ts, -2.28e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Loading configuration... done.\n",
      "Result for PPO_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_11-53-58\n",
      "  done: false\n",
      "  episode_len_mean: 1000.0\n",
      "  episode_reward_max: -22200.0\n",
      "  episode_reward_mean: -22750.166666666668\n",
      "  episode_reward_min: -23286.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 6\n",
      "  experiment_id: 18bfee7cff51456fa8e7b1a0fab8d1fe\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 117.471\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0062500000931322575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.3856301307678223\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 4.81330935144797e-05\n",
      "        policy_loss: -0.00010771179222501814\n",
      "        total_loss: 434938.53125\n",
      "        vf_explained_var: 0.00016808509826660156\n",
      "        vf_loss: 434938.5625\n",
      "    load_time_ms: 19.09\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "    sample_time_ms: 10136.547\n",
      "    update_time_ms: 0.004\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.299999999999997\n",
      "    ram_util_percent: 40.693749999999994\n",
      "  pid: 6494\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 8.237806166886969\n",
      "    mean_inference_ms: 0.838410561282582\n",
      "    mean_processing_ms: 1.2032386605164473\n",
      "  time_since_restore: 61.69715070724487\n",
      "  time_this_iter_s: 10.034720420837402\n",
      "  time_total_s: 61.69715070724487\n",
      "  timestamp: 1585734838\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 1000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: 8bdc7078\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.3/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=6494], 61 s, 6 iter, 6000 ts, -2.28e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Teleporting vehicle 'flow_10.63'; collision with vehicle 'flow_20.67', lane=':right_junction_8_0', gap=-1.00, time=802.00 stage=move.\n",
      "\u001b[2m\u001b[36m(pid=6494)\u001b[0m Warning: Vehicle 'flow_10.63' ends teleporting on edge '-gneE28', time 802.00.\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
