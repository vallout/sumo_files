{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments, run\n",
    "from ray.tune.experiment import Experiment\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 11:30:17,974\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-10_11-30-17_973354_6857/logs.\n",
      "2020-03-10 11:30:18,088\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:36406 to respond...\n",
      "2020-03-10 11:30:18,236\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:17150 to respond...\n",
      "2020-03-10 11:30:18,247\tINFO services.py:809 -- Starting Redis shard with 1.65 GB max memory.\n",
      "2020-03-10 11:30:18,299\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-03-10_11-30-17_973354_6857/logs.\n",
      "2020-03-10 11:30:18,302\tWARNING services.py:1330 -- WARNING: The default object store size of 2.47 GB will use more than 50% of the available memory on this node (4.73 GB). Consider setting the object store memory manually to a smaller size to avoid memory contention with other applications.\n",
      "2020-03-10 11:30:18,304\tINFO services.py:1475 -- Starting the Plasma object store with 2.47 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.105',\n",
       " 'redis_address': '192.168.2.105:36406',\n",
       " 'object_store_address': '/tmp/ray/session_2020-03-10_11-30-17_973354_6857/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-03-10_11-30-17_973354_6857/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-03-10_11-30-17_973354_6857'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 4\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(num_cpus=N_CPUS)#, object_store_memory=1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(config):\n",
    "    # ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
    "    # ensure we run at least one sgd iter\n",
    "    if config[\"num_sgd_iter\"] < 1:\n",
    "        config[\"num_sgd_iter\"] = 1\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=4,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            \"vf_clip_param\": lambda: random.uniform(20000, 50000),\n",
    "            \"lr\": [5e-2, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(N_CPUS*HORIZON, 2*N_CPUS*HORIZON),\n",
    "        },\n",
    "        custom_explore_fn=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "# #      \"class registered in the tune registry.\")\n",
    "alg_run = \"DQN\"\n",
    "\n",
    "# HORIZON = 200\n",
    "BATCH_SIZE = 200\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"num_envs_per_worker\"] = 1  # number of parallel workers\n",
    "# config[\"num_gpus\"] = 0.1\n",
    "config[\"train_batch_size\"] = 1000  # batch size\n",
    "config[\"sample_batch_size\"] = 200  # batch size\n",
    "config[\"gamma\"] = 0.99  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [256]})  # size of hidden layers in network\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "config[\"n_step\"] = 2\n",
    "# config[\"noisy\"] = True\n",
    "config[\"num_atoms\"] = 2\n",
    "\n",
    "# save the flow params for replay\n",
    "# flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "#                        indent=4)  # generating a string version of flow_params\n",
    "# config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "# config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "# create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "config[\"env\"] = \"CartPole-v1\"\n",
    "# Register as rllib env with Gym\n",
    "# register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(\"cart_pole_tests\", **{\n",
    "        \"run\": alg_run,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 5,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 5,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"episode_reward_mean\": 200,  # number of iterations to stop after\n",
    "        },\n",
    "        \"num_samples\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-10 11:30:18,572\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-03-10 11:30:18,628\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-03-10 11:30:18,641\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-03-10 11:30:18,643\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2020-03-10 11:30:18,750\tWARNING util.py:145 -- The `start_trial` operation took 0.1493391990661621 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.6/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,251\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,251\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21.251885: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21.274252: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21.274652: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x422e550 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21.274671: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,275\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,275\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd4c6bde400>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,491\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fd4c4202d68>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,495\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fd4c42027b8>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,590\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,593\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fd4c411a8d0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,630\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fd4c40eb5f8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:21,668\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fd4c4058fd0>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,065\tDEBUG tf_policy.py:214 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,227\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,227\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,228\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,229\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,230\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,230\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,231\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,232\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,232\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,233\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,234\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fd4c6bde6a0>}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,234\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fd4c6bde400>}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,234\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fd4c6bbd550>}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,235\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fd4c6bde2b0> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fd4c6bde6a0>}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,264\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:22,328\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 0}\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,519\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,519\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24.520046: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24.542119: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24.542323: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b9dbb0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24.542345: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,542\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,543\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f1bc598eba8>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,830\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1bb47e6518>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,834\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1bb47e60f0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24,889\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24,889\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24.889912: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,935\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1bb46c2be0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24,909\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24,909\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24.909872: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24.916056: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24.916229: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c58d00 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24.916249: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24,916\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:24,917\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f3d18365be0>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24.898231: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24.898399: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x507e460 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24.898421: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24,898\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:24,899\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7f135a30bba8>: Box(4,) -> (4,)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:24,977\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1bb469e9e8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Please use `layer.__call__` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,017\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1bb4622470>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:25,218\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f3d141f4550>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:25,222\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f3d141f40f0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,242\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f13581a04a8>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,248\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f13581516a0>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:25,370\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f3d140d0c50>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,395\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,400\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f13580a5080>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:25,437\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f3d140ab9e8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,444\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f1358058d30>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:25,502\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f3d1402f470>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:25,497\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7f135079e748>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(4,), Discrete(2), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 256), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Use `tf.cast` instead.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Future major versions of TensorFlow will allow gradients to flow\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m into the labels input on backprop by default.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,919\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,920\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,921\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,922\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,923\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,924\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,925\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,926\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,927\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,928\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m 2020-03-10 11:30:25,930\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f1bc598ea58> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f1bc598ee48>}\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6897)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,070\tDEBUG tf_policy.py:214 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,344\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,346\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,348\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,349\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,351\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,352\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,354\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,355\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,357\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,358\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,361\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f135a30ba58> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f135a30be48>}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,470\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,471\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,472\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,473\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,474\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,475\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,475\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,476\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,448\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,449\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.036, max=0.047, mean=0.004)}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,449\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,449\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.036, max=0.047, mean=0.004)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,449\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.036, max=0.047, mean=0.004)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,450\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.036, max=0.047, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,450\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,477\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,479\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m 2020-03-10 11:30:26,482\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7f3d18365a90> (<TimeLimit<CartPoleEnv<CartPole-v1>>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7f3d18365e80>}\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=6898)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,493\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=-0.264, max=-0.129, mean=-0.196)})}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,522\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((31,), dtype=int64, min=0.0, max=1.0, mean=0.516),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'agent_index': np.ndarray((31,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'dones': np.ndarray((31,), dtype=bool, min=0.0, max=1.0, mean=0.065),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'eps_id': np.ndarray((31,), dtype=int64, min=647268367.0, max=647268367.0, mean=647268367.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'infos': np.ndarray((31,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'new_obs': np.ndarray((31, 4), dtype=float32, min=-1.534, max=0.533, mean=-0.134),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'obs': np.ndarray((31, 4), dtype=float32, min=-1.185, max=0.533, mean=-0.111),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_actions': np.ndarray((31,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_rewards': np.ndarray((31,), dtype=float32, min=0.0, max=1.0, mean=0.968),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'q_values': np.ndarray((31, 2), dtype=float32, min=-3.158, max=0.346, mean=-0.601),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'rewards': np.ndarray((31,), dtype=float32, min=1.0, max=1.99, mean=1.958),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         't': np.ndarray((31,), dtype=int64, min=0.0, max=30.0, mean=15.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'unroll_id': np.ndarray((31,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'weights': np.ndarray((31,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:30:26,688\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.535),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.07),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=228857962.0, max=1802118403.0, mean=815217673.965),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.481, max=2.046, mean=-0.042),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.481, max=1.85, mean=-0.034),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.515),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.96),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=-3.158, max=0.346, mean=-0.76),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.99, mean=1.95),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=60.0, mean=16.67),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-27\n",
      "  done: false\n",
      "  episode_len_mean: 20.857142857142858\n",
      "  episode_reward_max: 61.0\n",
      "  episode_reward_mean: 20.857142857142858\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 56\n",
      "  episodes_total: 56\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: .nan\n",
      "    learner: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 1200\n",
      "    num_steps_trained: 0\n",
      "    num_target_updates: 2\n",
      "    opt_peak_throughput: 0.0\n",
      "    opt_samples: .nan\n",
      "    replay_time_ms: .nan\n",
      "    sample_time_ms: 2162.917\n",
      "    update_time_ms: 226.604\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.48571428571428\n",
      "    ram_util_percent: 50.41428571428571\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.06883140981176122\n",
      "    mean_inference_ms: 0.9645059370340553\n",
      "    mean_processing_ms: 0.1267699805450983\n",
      "  time_since_restore: 4.824115753173828\n",
      "  time_this_iter_s: 4.824115753173828\n",
      "  time_total_s: 4.824115753173828\n",
      "  timestamp: 1583836227\n",
      "  timesteps_since_restore: 1200\n",
      "  timesteps_this_iter: 1200\n",
      "  timesteps_total: 1200\n",
      "  training_iteration: 1\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.4/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 4 s, 1 iter, 1200 ts, 20.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m /usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   out=out, **kwargs)\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m /usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   ret = ret.dtype.type(ret / rcount)\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,160\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 1200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'count': 1000,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.482),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'batch_indexes': np.ndarray((1000,), dtype=int64, min=0.0, max=1796.0, mean=907.698),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.09),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'new_obs': np.ndarray((1000, 4), dtype=float32, min=-2.532, max=2.844, mean=-0.003),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'obs': np.ndarray((1000, 4), dtype=float32, min=-2.481, max=2.496, mean=-0.004),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.99, mean=1.942),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'weights': np.ndarray((1000,), dtype=float64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,522\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,523\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,523\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,523\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:27,674\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'mean_td_error': 0.70505035,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                       'td_error': np.ndarray((1000,), dtype=float32, min=0.679, max=0.913, mean=0.705)}}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:28,183\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 2400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:29,502\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 4200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:30,549\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 5400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:31,704\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 6600}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-32\n",
      "  done: false\n",
      "  episode_len_mean: 14.9\n",
      "  episode_reward_max: 33.0\n",
      "  episode_reward_mean: 14.9\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 76\n",
      "  episodes_total: 485\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 87.184\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5858601331710815\n",
      "        model: {}\n",
      "    max_exploration: 0.35319999999999996\n",
      "    min_exploration: 0.35319999999999996\n",
      "    num_steps_sampled: 7800\n",
      "    num_steps_trained: 11000\n",
      "    num_target_updates: 13\n",
      "    opt_peak_throughput: 11469.935\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 83.509\n",
      "    sample_time_ms: 319.079\n",
      "    update_time_ms: 5.747\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.8\n",
      "    ram_util_percent: 54.1\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07486134567498455\n",
      "    mean_inference_ms: 1.0264769693147753\n",
      "    mean_processing_ms: 0.14185103872205257\n",
      "  time_since_restore: 10.406500577926636\n",
      "  time_this_iter_s: 1.0645322799682617\n",
      "  time_total_s: 10.406500577926636\n",
      "  timestamp: 1583836232\n",
      "  timesteps_since_restore: 7800\n",
      "  timesteps_this_iter: 1200\n",
      "  timesteps_total: 7800\n",
      "  training_iteration: 6\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 10 s, 6 iter, 7800 ts, 14.9 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:32,785\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 7800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:33,888\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 9000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:35,013\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 10200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:36,429\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 12000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:37,577\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 13200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-38\n",
      "  done: false\n",
      "  episode_len_mean: 19.57\n",
      "  episode_reward_max: 43.0\n",
      "  episode_reward_mean: 19.57\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 65\n",
      "  episodes_total: 947\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 88.242\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5721501708030701\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 14400\n",
      "    num_steps_trained: 22000\n",
      "    num_target_updates: 24\n",
      "    opt_peak_throughput: 11332.532\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 88.424\n",
      "    sample_time_ms: 352.361\n",
      "    update_time_ms: 5.484\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.4\n",
      "    ram_util_percent: 54.7\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07889507534979157\n",
      "    mean_inference_ms: 1.0791919224998918\n",
      "    mean_processing_ms: 0.15202183462729982\n",
      "  time_since_restore: 16.32404088973999\n",
      "  time_this_iter_s: 1.1589701175689697\n",
      "  time_total_s: 16.32404088973999\n",
      "  timestamp: 1583836238\n",
      "  timesteps_since_restore: 14400\n",
      "  timesteps_this_iter: 1200\n",
      "  timesteps_total: 14400\n",
      "  training_iteration: 11\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 16 s, 11 iter, 14400 ts, 19.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:38,748\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 14400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:39,773\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 15600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:41,068\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 17400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:42,398\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 19200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-43\n",
      "  done: false\n",
      "  episode_len_mean: 12.54861111111111\n",
      "  episode_reward_max: 27.0\n",
      "  episode_reward_mean: 12.54861111111111\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 144\n",
      "  episodes_total: 1476\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 78.467\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5728113055229187\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 33000\n",
      "    num_target_updates: 35\n",
      "    opt_peak_throughput: 12744.252\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 79.061\n",
      "    sample_time_ms: 281.304\n",
      "    update_time_ms: 5.244\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.8\n",
      "    ram_util_percent: 54.9\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07663409212236509\n",
      "    mean_inference_ms: 1.0358338647641798\n",
      "    mean_processing_ms: 0.14830243264161236\n",
      "  time_since_restore: 21.39311671257019\n",
      "  time_this_iter_s: 1.4325692653656006\n",
      "  time_total_s: 21.39311671257019\n",
      "  timestamp: 1583836243\n",
      "  timesteps_since_restore: 21000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 15\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.5/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 21 s, 15 iter, 21000 ts, 12.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:43,848\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 21000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:45,404\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 22800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:46,806\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 24600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:48,246\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 26400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-49\n",
      "  done: false\n",
      "  episode_len_mean: 21.6\n",
      "  episode_reward_max: 32.0\n",
      "  episode_reward_mean: 21.6\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1933\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 80.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5743201971054077\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 28200\n",
      "    num_steps_trained: 45000\n",
      "    num_target_updates: 47\n",
      "    opt_peak_throughput: 12394.318\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 83.579\n",
      "    sample_time_ms: 308.199\n",
      "    update_time_ms: 6.327\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.85\n",
      "    ram_util_percent: 55.4\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07632070210712155\n",
      "    mean_inference_ms: 1.0298868187067658\n",
      "    mean_processing_ms: 0.1477939019507329\n",
      "  time_since_restore: 27.190902948379517\n",
      "  time_this_iter_s: 1.414780855178833\n",
      "  time_total_s: 27.190902948379517\n",
      "  timestamp: 1583836249\n",
      "  timesteps_since_restore: 28200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 28200\n",
      "  training_iteration: 19\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 27 s, 19 iter, 28200 ts, 21.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:49,668\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 28200}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:50,729\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 29400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:52,180\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 31200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:53,563\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 33000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:54,608\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 34200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-30-55\n",
      "  done: false\n",
      "  episode_len_mean: 13.11111111111111\n",
      "  episode_reward_max: 24.0\n",
      "  episode_reward_mean: 13.11111111111111\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 135\n",
      "  episodes_total: 2460\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 77.645\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5829382538795471\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 58000\n",
      "    num_target_updates: 60\n",
      "    opt_peak_throughput: 12879.083\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 83.545\n",
      "    sample_time_ms: 310.822\n",
      "    update_time_ms: 5.088\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.9\n",
      "    ram_util_percent: 55.75\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07516214212695682\n",
      "    mean_inference_ms: 1.0151739757714326\n",
      "    mean_processing_ms: 0.14615726106285526\n",
      "  time_since_restore: 33.479543924331665\n",
      "  time_this_iter_s: 1.3739590644836426\n",
      "  time_total_s: 33.479543924331665\n",
      "  timestamp: 1583836255\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 24\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 33 s, 24 iter, 36000 ts, 13.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:55,989\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 36000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:57,305\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 37800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:58,625\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 39600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:30:59,748\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 40800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-01\n",
      "  done: false\n",
      "  episode_len_mean: 15.586206896551724\n",
      "  episode_reward_max: 27.0\n",
      "  episode_reward_mean: 15.586206896551724\n",
      "  episode_reward_min: 9.0\n",
      "  episodes_this_iter: 116\n",
      "  episodes_total: 2920\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 79.453\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5352728366851807\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 42600\n",
      "    num_steps_trained: 69000\n",
      "    num_target_updates: 71\n",
      "    opt_peak_throughput: 12586.081\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 80.08\n",
      "    sample_time_ms: 289.058\n",
      "    update_time_ms: 5.61\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.2\n",
      "    ram_util_percent: 56.2\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.0743184287039728\n",
      "    mean_inference_ms: 1.0017240863653871\n",
      "    mean_processing_ms: 0.14431152903826322\n",
      "  time_since_restore: 38.510693311691284\n",
      "  time_this_iter_s: 1.2924351692199707\n",
      "  time_total_s: 38.510693311691284\n",
      "  timestamp: 1583836261\n",
      "  timesteps_since_restore: 42600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 42600\n",
      "  training_iteration: 28\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 38 s, 28 iter, 42600 ts, 15.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:01,049\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 42600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:02,475\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 44400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:03,781\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 46200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:05,275\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 48000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-06\n",
      "  done: false\n",
      "  episode_len_mean: 13.834645669291339\n",
      "  episode_reward_max: 30.0\n",
      "  episode_reward_mean: 13.834645669291339\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 127\n",
      "  episodes_total: 3488\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 79.998\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5565665364265442\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 49800\n",
      "    num_steps_trained: 81000\n",
      "    num_target_updates: 83\n",
      "    opt_peak_throughput: 12500.273\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 83.068\n",
      "    sample_time_ms: 294.398\n",
      "    update_time_ms: 5.189\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 70.9\n",
      "    ram_util_percent: 56.349999999999994\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07377267687030986\n",
      "    mean_inference_ms: 0.9924419317305839\n",
      "    mean_processing_ms: 0.14380593779155232\n",
      "  time_since_restore: 44.11871290206909\n",
      "  time_this_iter_s: 1.4033029079437256\n",
      "  time_total_s: 44.11871290206909\n",
      "  timestamp: 1583836266\n",
      "  timesteps_since_restore: 49800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 49800\n",
      "  training_iteration: 32\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.6/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 44 s, 32 iter, 49800 ts, 13.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:06,686\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 49800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:08,081\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 51600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:09,389\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 53400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:10,688\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 55200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 12.55\n",
      "  episode_reward_max: 55.0\n",
      "  episode_reward_mean: 12.55\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 140\n",
      "  episodes_total: 3918\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 81.484\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5418348908424377\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 93000\n",
      "    num_target_updates: 95\n",
      "    opt_peak_throughput: 12272.383\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 78.736\n",
      "    sample_time_ms: 279.761\n",
      "    update_time_ms: 4.891\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.65\n",
      "    ram_util_percent: 56.6\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07322520698559926\n",
      "    mean_inference_ms: 0.9836028368756432\n",
      "    mean_processing_ms: 0.14230214633577554\n",
      "  time_since_restore: 49.55892753601074\n",
      "  time_this_iter_s: 1.4591407775878906\n",
      "  time_total_s: 49.55892753601074\n",
      "  timestamp: 1583836272\n",
      "  timesteps_since_restore: 57000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 36\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 49 s, 36 iter, 57000 ts, 12.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:12,155\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 57000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:13,528\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 58800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:14,821\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 60600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:16,106\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 62400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-17\n",
      "  done: false\n",
      "  episode_len_mean: 73.63\n",
      "  episode_reward_max: 165.0\n",
      "  episode_reward_mean: 73.63\n",
      "  episode_reward_min: 11.0\n",
      "  episodes_this_iter: 33\n",
      "  episodes_total: 4014\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 76.753\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.5099105834960938\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 64200\n",
      "    num_steps_trained: 105000\n",
      "    num_target_updates: 107\n",
      "    opt_peak_throughput: 13028.852\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 76.169\n",
      "    sample_time_ms: 265.208\n",
      "    update_time_ms: 5.179\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.0\n",
      "    ram_util_percent: 56.8\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07275015428140864\n",
      "    mean_inference_ms: 0.976500475837483\n",
      "    mean_processing_ms: 0.14031154052429304\n",
      "  time_since_restore: 54.78079319000244\n",
      "  time_this_iter_s: 1.28257155418396\n",
      "  time_total_s: 54.78079319000244\n",
      "  timestamp: 1583836277\n",
      "  timesteps_since_restore: 64200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 64200\n",
      "  training_iteration: 40\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 54 s, 40 iter, 64200 ts, 73.6 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:17,402\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 64200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:18,853\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 66000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:19,925\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 67200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:21,206\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 69000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-22\n",
      "  done: false\n",
      "  episode_len_mean: 81.11\n",
      "  episode_reward_max: 211.0\n",
      "  episode_reward_mean: 81.11\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_total: 4085\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 82.255\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.4809621572494507\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 70800\n",
      "    num_steps_trained: 116000\n",
      "    num_target_updates: 118\n",
      "    opt_peak_throughput: 12157.287\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 82.716\n",
      "    sample_time_ms: 305.269\n",
      "    update_time_ms: 5.306\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.5\n",
      "    ram_util_percent: 57.2\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07251228127184879\n",
      "    mean_inference_ms: 0.971065498728465\n",
      "    mean_processing_ms: 0.13846792722929946\n",
      "  time_since_restore: 60.01483631134033\n",
      "  time_this_iter_s: 1.4452412128448486\n",
      "  time_total_s: 60.01483631134033\n",
      "  timestamp: 1583836282\n",
      "  timesteps_since_restore: 70800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 70800\n",
      "  training_iteration: 44\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 60 s, 44 iter, 70800 ts, 81.1 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:22,664\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 70800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:24,047\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 72600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:25,448\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 74400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:26,830\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 76200}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,843\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-0.483, max=0.165, mean=-0.084),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_action': 1,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,843\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,844\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=5.169, max=5.667, mean=5.418)})}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,844\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-0.217, max=0.065, mean=-0.068)}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,844\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,845\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-0.217, max=0.065, mean=-0.068)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,845\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-0.217, max=0.065, mean=-0.068)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:26,857\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.077),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'agent_index': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'dones': np.ndarray((13,), dtype=bool, min=0.0, max=1.0, mean=0.154),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'eps_id': np.ndarray((13,), dtype=int64, min=1052537332.0, max=1052537332.0, mean=1052537332.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'infos': np.ndarray((13,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'new_obs': np.ndarray((13, 4), dtype=float32, min=-2.168, max=2.88, mean=0.067),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'obs': np.ndarray((13, 4), dtype=float32, min=-2.168, max=2.88, mean=0.029),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_actions': np.ndarray((13,), dtype=int64, min=0.0, max=1.0, mean=0.077),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_rewards': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'q_values': np.ndarray((13, 2), dtype=float32, min=1.591, max=7.783, mean=5.573),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'rewards': np.ndarray((13,), dtype=float32, min=1.0, max=1.99, mean=1.914),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         't': np.ndarray((13,), dtype=int64, min=13.0, max=25.0, mean=19.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'unroll_id': np.ndarray((13,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'weights': np.ndarray((13,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:27,059\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.3),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.08),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=453332911.0, max=1699017408.0, mean=1144709687.31),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.363, max=3.173, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.363, max=3.173, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.265),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.96),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=0.801, max=7.843, mean=5.705),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.99, mean=1.945),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=28.0, mean=12.35),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=127.0, max=127.0, mean=127.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:31:27,319\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,722\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:27,740\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'mean_td_error': 0.45568871,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                       'td_error': np.ndarray((1000,), dtype=float32, min=0.123, max=1.326, mean=0.456)}}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:28,234\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'count': 1000,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.539),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'batch_indexes': np.ndarray((1000,), dtype=int64, min=208.0, max=49981.0, mean=24226.132),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.102),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'new_obs': np.ndarray((1000, 4), dtype=float32, min=-3.68, max=3.587, mean=-0.133),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'obs': np.ndarray((1000, 4), dtype=float32, min=-3.002, max=3.244, mean=-0.118),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.99, mean=1.935),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'weights': np.ndarray((1000,), dtype=float64, min=0.465, max=0.868, mean=0.546)},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-28\n",
      "  done: false\n",
      "  episode_len_mean: 35.76\n",
      "  episode_reward_max: 250.0\n",
      "  episode_reward_mean: 35.76\n",
      "  episode_reward_min: 15.0\n",
      "  episodes_this_iter: 29\n",
      "  episodes_total: 4271\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 83.654\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.45499107241630554\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 128000\n",
      "    num_target_updates: 130\n",
      "    opt_peak_throughput: 11954.059\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 80.189\n",
      "    sample_time_ms: 296.465\n",
      "    update_time_ms: 5.238\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.75\n",
      "    ram_util_percent: 57.45\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07250657838433537\n",
      "    mean_inference_ms: 0.9713845890459912\n",
      "    mean_processing_ms: 0.1369293226349414\n",
      "  time_since_restore: 65.64832806587219\n",
      "  time_this_iter_s: 1.4875450134277344\n",
      "  time_total_s: 65.64832806587219\n",
      "  timestamp: 1583836288\n",
      "  timesteps_since_restore: 78000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 48\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 65 s, 48 iter, 78000 ts, 35.8 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:28,326\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 78000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:29,829\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 79800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:30,894\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 81000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:32,201\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 82800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:33,281\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 84000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-34\n",
      "  done: false\n",
      "  episode_len_mean: 33.19\n",
      "  episode_reward_max: 153.0\n",
      "  episode_reward_mean: 33.19\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 71\n",
      "  episodes_total: 4414\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 80.372\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.39868950843811035\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 85800\n",
      "    num_steps_trained: 141000\n",
      "    num_target_updates: 143\n",
      "    opt_peak_throughput: 12442.148\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 80.67\n",
      "    sample_time_ms: 299.723\n",
      "    update_time_ms: 5.042\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.849999999999994\n",
      "    ram_util_percent: 57.5\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07254501487496585\n",
      "    mean_inference_ms: 0.9720365271155859\n",
      "    mean_processing_ms: 0.13584020630578275\n",
      "  time_since_restore: 71.84018778800964\n",
      "  time_this_iter_s: 1.264530897140503\n",
      "  time_total_s: 71.84018778800964\n",
      "  timestamp: 1583836294\n",
      "  timesteps_since_restore: 85800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 85800\n",
      "  training_iteration: 53\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 71 s, 53 iter, 85800 ts, 33.2 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:34,554\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 85800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:35,829\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 87600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:36,880\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 88800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:37,908\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 90000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:39,196\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 91800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-40\n",
      "  done: false\n",
      "  episode_len_mean: 85.48\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 85.48\n",
      "  episode_reward_min: 19.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4462\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 83.628\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.3863360285758972\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 93600\n",
      "    num_steps_trained: 154000\n",
      "    num_target_updates: 156\n",
      "    opt_peak_throughput: 11957.764\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 82.684\n",
      "    sample_time_ms: 288.854\n",
      "    update_time_ms: 5.124\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.25\n",
      "    ram_util_percent: 57.6\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07259309474824033\n",
      "    mean_inference_ms: 0.9693710327876591\n",
      "    mean_processing_ms: 0.13514548979779992\n",
      "  time_since_restore: 77.75146555900574\n",
      "  time_this_iter_s: 1.2923951148986816\n",
      "  time_total_s: 77.75146555900574\n",
      "  timestamp: 1583836300\n",
      "  timesteps_since_restore: 93600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 93600\n",
      "  training_iteration: 58\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.7/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 77 s, 58 iter, 93600 ts, 85.5 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:40,496\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 93600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:41,852\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 95400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:43,289\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 97200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:44,639\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 99000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 133.17\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 133.17\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4519\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 82.783\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.3121548891067505\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 100800\n",
      "    num_steps_trained: 166000\n",
      "    num_target_updates: 168\n",
      "    opt_peak_throughput: 12079.812\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 81.991\n",
      "    sample_time_ms: 273.237\n",
      "    update_time_ms: 5.568\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.599999999999994\n",
      "    ram_util_percent: 57.7\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07216439890503704\n",
      "    mean_inference_ms: 0.9667503617518688\n",
      "    mean_processing_ms: 0.1338184955001305\n",
      "  time_since_restore: 83.15471839904785\n",
      "  time_this_iter_s: 1.283048152923584\n",
      "  time_total_s: 83.15471839904785\n",
      "  timestamp: 1583836305\n",
      "  timesteps_since_restore: 100800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 100800\n",
      "  training_iteration: 62\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 83 s, 62 iter, 100800 ts, 133 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:45,931\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 100800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:47,216\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 102600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:48,248\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 103800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:49,668\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 105600}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-50\n",
      "  done: false\n",
      "  episode_len_mean: 109.18\n",
      "  episode_reward_max: 142.0\n",
      "  episode_reward_mean: 109.18\n",
      "  episode_reward_min: 83.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_total: 4579\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 81.145\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.2926517724990845\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 107400\n",
      "    num_steps_trained: 177000\n",
      "    num_target_updates: 179\n",
      "    opt_peak_throughput: 12323.611\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 81.022\n",
      "    sample_time_ms: 285.332\n",
      "    update_time_ms: 5.208\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.8\n",
      "    ram_util_percent: 58.05\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07194879541521276\n",
      "    mean_inference_ms: 0.9620195959200689\n",
      "    mean_processing_ms: 0.13244673766018045\n",
      "  time_since_restore: 88.1493399143219\n",
      "  time_this_iter_s: 1.2809641361236572\n",
      "  time_total_s: 88.1493399143219\n",
      "  timestamp: 1583836310\n",
      "  timesteps_since_restore: 107400\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 107400\n",
      "  training_iteration: 66\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 88 s, 66 iter, 107400 ts, 109 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:50,957\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 107400}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:52,291\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 109200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:53,344\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 110400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:54,853\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 112200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 108.38\n",
      "  episode_reward_max: 142.0\n",
      "  episode_reward_mean: 108.38\n",
      "  episode_reward_min: 61.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_total: 4641\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 82.292\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.2639786899089813\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 188000\n",
      "    num_target_updates: 190\n",
      "    opt_peak_throughput: 12151.838\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 81.533\n",
      "    sample_time_ms: 297.698\n",
      "    update_time_ms: 6.381\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.3\n",
      "    ram_util_percent: 58.1\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07186186541440383\n",
      "    mean_inference_ms: 0.9600531826306172\n",
      "    mean_processing_ms: 0.13163598797581785\n",
      "  time_since_restore: 93.30210041999817\n",
      "  time_this_iter_s: 1.2698395252227783\n",
      "  time_total_s: 93.30210041999817\n",
      "  timestamp: 1583836316\n",
      "  timesteps_since_restore: 114000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 70\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 93 s, 70 iter, 114000 ts, 108 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:56,140\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 114000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:57,516\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 115800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:31:59,037\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 117600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:00,407\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 119400}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-01\n",
      "  done: false\n",
      "  episode_len_mean: 97.03\n",
      "  episode_reward_max: 133.0\n",
      "  episode_reward_mean: 97.03\n",
      "  episode_reward_min: 61.0\n",
      "  episodes_this_iter: 21\n",
      "  episodes_total: 4716\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 81.168\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.24336117506027222\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 121200\n",
      "    num_steps_trained: 200000\n",
      "    num_target_updates: 202\n",
      "    opt_peak_throughput: 12320.125\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 83.1\n",
      "    sample_time_ms: 288.148\n",
      "    update_time_ms: 5.499\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.6\n",
      "    ram_util_percent: 58.5\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07181019687615123\n",
      "    mean_inference_ms: 0.959431510702515\n",
      "    mean_processing_ms: 0.13086478377123856\n",
      "  time_since_restore: 98.82385063171387\n",
      "  time_this_iter_s: 1.2687444686889648\n",
      "  time_total_s: 98.82385063171387\n",
      "  timestamp: 1583836321\n",
      "  timesteps_since_restore: 121200\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 121200\n",
      "  training_iteration: 74\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 98 s, 74 iter, 121200 ts, 97 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:01,684\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 121200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:02,994\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 123000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:04,147\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 124200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:05,487\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 126000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 103.37\n",
      "  episode_reward_max: 146.0\n",
      "  episode_reward_mean: 103.37\n",
      "  episode_reward_min: 68.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_total: 4773\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 82.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.26004186272621155\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 127800\n",
      "    num_steps_trained: 211000\n",
      "    num_target_updates: 213\n",
      "    opt_peak_throughput: 12124.568\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 82.009\n",
      "    sample_time_ms: 287.945\n",
      "    update_time_ms: 5.783\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.2\n",
      "    ram_util_percent: 58.6\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07167851996389796\n",
      "    mean_inference_ms: 0.9574935572008145\n",
      "    mean_processing_ms: 0.13029439690804345\n",
      "  time_since_restore: 103.87507200241089\n",
      "  time_this_iter_s: 1.276838779449463\n",
      "  time_total_s: 103.87507200241089\n",
      "  timestamp: 1583836326\n",
      "  timesteps_since_restore: 127800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 127800\n",
      "  training_iteration: 78\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 103 s, 78 iter, 127800 ts, 103 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:06,772\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 127800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:08,166\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 129600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:09,452\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 131400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:10,728\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 133200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-11\n",
      "  done: false\n",
      "  episode_len_mean: 121.9\n",
      "  episode_reward_max: 166.0\n",
      "  episode_reward_mean: 121.9\n",
      "  episode_reward_min: 98.0\n",
      "  episodes_this_iter: 14\n",
      "  episodes_total: 4832\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 76.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.22084543108940125\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 135000\n",
      "    num_steps_trained: 223000\n",
      "    num_target_updates: 225\n",
      "    opt_peak_throughput: 12999.29\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 76.292\n",
      "    sample_time_ms: 262.282\n",
      "    update_time_ms: 5.174\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.3\n",
      "    ram_util_percent: 58.849999999999994\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07152185334339432\n",
      "    mean_inference_ms: 0.9550659601917283\n",
      "    mean_processing_ms: 0.1295982689119495\n",
      "  time_since_restore: 109.0788950920105\n",
      "  time_this_iter_s: 1.267357587814331\n",
      "  time_total_s: 109.0788950920105\n",
      "  timestamp: 1583836331\n",
      "  timesteps_since_restore: 135000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 135000\n",
      "  training_iteration: 82\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 109 s, 82 iter, 135000 ts, 122 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:12,004\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 135000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:13,280\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 136800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:14,331\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 138000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:15,742\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 139800}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-17\n",
      "  done: false\n",
      "  episode_len_mean: 131.86\n",
      "  episode_reward_max: 170.0\n",
      "  episode_reward_mean: 131.86\n",
      "  episode_reward_min: 98.0\n",
      "  episodes_this_iter: 12\n",
      "  episodes_total: 4878\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 79.799\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.22711428999900818\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 141600\n",
      "    num_steps_trained: 234000\n",
      "    num_target_updates: 236\n",
      "    opt_peak_throughput: 12531.413\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 81.1\n",
      "    sample_time_ms: 294.013\n",
      "    update_time_ms: 5.488\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.95\n",
      "    ram_util_percent: 59.0\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07139213299212818\n",
      "    mean_inference_ms: 0.9518221525034918\n",
      "    mean_processing_ms: 0.1289182132321421\n",
      "  time_since_restore: 114.14772987365723\n",
      "  time_this_iter_s: 1.3528761863708496\n",
      "  time_total_s: 114.14772987365723\n",
      "  timestamp: 1583836337\n",
      "  timesteps_since_restore: 141600\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 141600\n",
      "  training_iteration: 86\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 114 s, 86 iter, 141600 ts, 132 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:17,103\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 141600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:18,405\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 143400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:19,678\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 145200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:20,947\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 147000}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-22\n",
      "  done: false\n",
      "  episode_len_mean: 154.45\n",
      "  episode_reward_max: 210.0\n",
      "  episode_reward_mean: 154.45\n",
      "  episode_reward_min: 106.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 4919\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 76.879\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.22751514613628387\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 148800\n",
      "    num_steps_trained: 246000\n",
      "    num_target_updates: 248\n",
      "    opt_peak_throughput: 13007.53\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 76.294\n",
      "    sample_time_ms: 261.753\n",
      "    update_time_ms: 4.76\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.1\n",
      "    ram_util_percent: 59.3\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07123336953358082\n",
      "    mean_inference_ms: 0.9499754255159385\n",
      "    mean_processing_ms: 0.12832392423096922\n",
      "  time_since_restore: 119.2526924610138\n",
      "  time_this_iter_s: 1.2729976177215576\n",
      "  time_total_s: 119.2526924610138\n",
      "  timestamp: 1583836342\n",
      "  timesteps_since_restore: 148800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 148800\n",
      "  training_iteration: 90\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 119 s, 90 iter, 148800 ts, 154 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:22,234\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 148800}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:23,505\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 150600}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:24,814\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 152400}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:25,969\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 153600}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 176.41\n",
      "  episode_reward_max: 230.0\n",
      "  episode_reward_mean: 176.41\n",
      "  episode_reward_min: 106.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 4949\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 81.921\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.1938341110944748\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 155400\n",
      "    num_steps_trained: 257000\n",
      "    num_target_updates: 259\n",
      "    opt_peak_throughput: 12206.878\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 80.571\n",
      "    sample_time_ms: 284.834\n",
      "    update_time_ms: 5.696\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.7\n",
      "    ram_util_percent: 59.5\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07116835355275712\n",
      "    mean_inference_ms: 0.9485488473018303\n",
      "    mean_processing_ms: 0.12784956039894532\n",
      "  time_since_restore: 124.24827671051025\n",
      "  time_this_iter_s: 1.2748444080352783\n",
      "  time_total_s: 124.24827671051025\n",
      "  timestamp: 1583836347\n",
      "  timesteps_since_restore: 155400\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 155400\n",
      "  training_iteration: 94\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 124 s, 94 iter, 155400 ts, 176 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:27,252\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 155400}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,320\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((4,), dtype=float64, min=-1.625, max=0.214, mean=-0.598)}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,320\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,320\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((4,), dtype=float64, min=-1.625, max=0.214, mean=-0.598)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,320\tINFO sampler.py:407 -- Filtered obs: np.ndarray((4,), dtype=float64, min=-1.625, max=0.214, mean=-0.598)\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,321\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'obs': np.ndarray((4,), dtype=float64, min=-1.625, max=0.214, mean=-0.598),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_action': 0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'prev_reward': 1.0,\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,321\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,322\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                       { 'q_values': np.ndarray((1, 2), dtype=float32, min=7.724, max=7.813, mean=7.768)})}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,374\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((95,), dtype=int64, min=0.0, max=1.0, mean=0.495),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'agent_index': np.ndarray((95,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'dones': np.ndarray((95,), dtype=bool, min=0.0, max=1.0, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'eps_id': np.ndarray((95,), dtype=int64, min=285505046.0, max=285505046.0, mean=285505046.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'infos': np.ndarray((95,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'new_obs': np.ndarray((95, 4), dtype=float32, min=-2.409, max=0.629, mean=-0.608),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'obs': np.ndarray((95, 4), dtype=float32, min=-2.386, max=0.629, mean=-0.597),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_actions': np.ndarray((95,), dtype=int64, min=0.0, max=1.0, mean=0.484),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'prev_rewards': np.ndarray((95,), dtype=float32, min=1.0, max=1.0, mean=1.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'q_values': np.ndarray((95, 2), dtype=float32, min=6.545, max=8.813, mean=7.819),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'rewards': np.ndarray((95,), dtype=float32, min=1.0, max=1.99, mean=1.98),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         't': np.ndarray((95,), dtype=int64, min=108.0, max=202.0, mean=155.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'unroll_id': np.ndarray((95,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m                         'weights': np.ndarray((95,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,498\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m { 'data': { 'actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.49),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=285505046.0, max=1323685361.0, mean=830549711.375),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'new_obs': np.ndarray((200, 4), dtype=float32, min=-2.409, max=0.629, mean=-0.37),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'obs': np.ndarray((200, 4), dtype=float32, min=-2.386, max=0.629, mean=-0.362),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=1.0, mean=0.485),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=0.0, max=1.0, mean=0.995),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'q_values': np.ndarray((200, 2), dtype=float32, min=6.545, max=9.788, mean=8.707),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=1.0, max=1.99, mean=1.98),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=202.0, mean=100.925),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=259.0, max=259.0, mean=259.0),\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m             'weights': np.ndarray((200,), dtype=float32, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6895)\u001b[0m 2020-03-10 11:32:27,705\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,473\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(256, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 2) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(2,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(4, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,474\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,490\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0005000000237487257,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'mean_td_error': 0.21183637,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                       'td_error': np.ndarray((1000,), dtype=float32, min=0.038, max=1.702, mean=0.212)}}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:28,568\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 157200}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:29,021\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m { 'count': 1000,\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((1000,), dtype=int64, min=0.0, max=1.0, mean=0.486),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'batch_indexes': np.ndarray((1000,), dtype=int64, min=73.0, max=49981.0, mean=23198.071),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'dones': np.ndarray((1000,), dtype=bool, min=0.0, max=1.0, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'new_obs': np.ndarray((1000, 4), dtype=float32, min=-2.406, max=2.554, mean=-0.318),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'obs': np.ndarray((1000, 4), dtype=float32, min=-2.4, max=2.208, mean=-0.311),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'rewards': np.ndarray((1000,), dtype=float32, min=1.0, max=1.99, mean=1.976),\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                                     'weights': np.ndarray((1000,), dtype=float64, min=0.357, max=0.904, mean=0.464)},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:29,962\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 159000}\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:31,084\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 160200}\n",
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-32\n",
      "  done: false\n",
      "  episode_len_mean: 198.74\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 198.74\n",
      "  episode_reward_min: 149.0\n",
      "  episodes_this_iter: 8\n",
      "  episodes_total: 4980\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 85.347\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.17713510990142822\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 268000\n",
      "    num_target_updates: 270\n",
      "    opt_peak_throughput: 11716.869\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 84.434\n",
      "    sample_time_ms: 312.168\n",
      "    update_time_ms: 5.061\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.1\n",
      "    ram_util_percent: 59.7\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07108006660850795\n",
      "    mean_inference_ms: 0.946520427530626\n",
      "    mean_processing_ms: 0.1273580567866132\n",
      "  time_since_restore: 129.59598875045776\n",
      "  time_this_iter_s: 1.5354688167572021\n",
      "  time_total_s: 129.59598875045776\n",
      "  timestamp: 1583836352\n",
      "  timesteps_since_restore: 162000\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 98\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 4/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_CartPole-v1_0:\tRUNNING, [4 CPUs, 0 GPUs], [pid=6896], 129 s, 98 iter, 162000 ts, 199 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=6896)\u001b[0m 2020-03-10 11:32:32,627\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 162000}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_CartPole-v1_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-03-10_11-32-34\n",
      "  done: true\n",
      "  episode_len_mean: 205.38\n",
      "  episode_reward_max: 316.0\n",
      "  episode_reward_mean: 205.38\n",
      "  episode_reward_min: 150.0\n",
      "  episodes_this_iter: 6\n",
      "  episodes_total: 4986\n",
      "  experiment_id: b16aa22f956249e0baca3de8c0962829\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 89.154\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0005000000237487257\n",
      "        mean_td_error: 0.16909317672252655\n",
      "        model: {}\n",
      "    max_exploration: 0.020000000000000018\n",
      "    min_exploration: 0.020000000000000018\n",
      "    num_steps_sampled: 163800\n",
      "    num_steps_trained: 271000\n",
      "    num_target_updates: 273\n",
      "    opt_peak_throughput: 11216.57\n",
      "    opt_samples: 1000.0\n",
      "    replay_time_ms: 89.004\n",
      "    sample_time_ms: 310.092\n",
      "    update_time_ms: 5.33\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.15\n",
      "    ram_util_percent: 59.7\n",
      "  pid: 6896\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 0.07105645544713977\n",
      "    mean_inference_ms: 0.9466754935707155\n",
      "    mean_processing_ms: 0.1272830009573798\n",
      "  time_since_restore: 131.05400919914246\n",
      "  time_this_iter_s: 1.4580204486846924\n",
      "  time_total_s: 131.05400919914246\n",
      "  timestamp: 1583836354\n",
      "  timesteps_since_restore: 163800\n",
      "  timesteps_this_iter: 1800\n",
      "  timesteps_total: 163800\n",
      "  training_iteration: 99\n",
      "  trial_id: 236be85c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/4 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/cart_pole_tests\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - DQN_CartPole-v1_0:\tTERMINATED, [4 CPUs, 0 GPUs], [pid=6896], 131 s, 99 iter, 163800 ts, 205 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
