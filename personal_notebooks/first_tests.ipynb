{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the TestEnv environment is used to simply simulate the network\n",
    "from flow.envs import TestEnv\n",
    "\n",
    "# the Experiment class is used for running simulations\n",
    "from flow.core.experiment import Experiment\n",
    "\n",
    "# the base network class\n",
    "from flow.networks import Network\n",
    "from flow.envs import Env\n",
    "\n",
    "# all other imports are standard\n",
    "from flow.core.params import VehicleParams\n",
    "from flow.core.params import NetParams\n",
    "from flow.core.params import InitialConfig\n",
    "from flow.core.params import EnvParams\n",
    "from flow.core.params import TrafficLightParams\n",
    "from flow.controllers import IDMController\n",
    "from flow.core.params import SumoCarFollowingParams\n",
    "\n",
    "# create some default parameters parameters\n",
    "HORIZON=2000\n",
    "env_params = EnvParams(horizon=HORIZON)\n",
    "initial_config = InitialConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_dir = \"/home/valentin/Schreibtisch/personal_sumo_files\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sim_params = SumoParams(render=True, sim_step=1, restart_instance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles=VehicleParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import InFlows\n",
    "\n",
    "inflow = InFlows()\n",
    "\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_east\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_south\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"right_north\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_north\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_south\",\n",
    "           probability=0.08)\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"left_west\",\n",
    "           probability=0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'flow_0',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_east',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_1',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_south',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_2',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'right_north',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_3',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_north',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_4',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_south',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08},\n",
       " {'name': 'flow_5',\n",
       "  'vtype': 'human',\n",
       "  'edge': 'left_west',\n",
       "  'departLane': 'first',\n",
       "  'departSpeed': 0,\n",
       "  'begin': 1,\n",
       "  'end': 86400,\n",
       "  'probability': 0.08}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inflow.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "net_params = NetParams(\n",
    "    inflows=inflow,\n",
    "    template={\n",
    "        # network geometry features\n",
    "        \"net\": os.path.join(le_dir, \"lemgo_small.net.xml\"),\n",
    "        # features associated with the properties of drivers\n",
    "        \"vtype\": os.path.join(le_dir, \"vtypes.add.xml\"),\n",
    "        # features associated with the routes vehicles take\n",
    "        \"rou\": os.path.join(le_dir, \"lemgo_small2_out.rou.xml\"),\n",
    "        \"det\": os.path.join(le_dir, \"lemgo_small.add.xml\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom network with lane area detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Running the Modified Simulation\n",
    "\n",
    "Finally, the fully imported simulation can be run as follows. \n",
    "\n",
    "**Warning**: the network takes time to initialize while the departure positions and times and vehicles are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the network\n",
    "network = Network(\n",
    "    name=\"template\",\n",
    "    net_params=net_params,\n",
    "    vehicles=vehicles\n",
    ")\n",
    "\n",
    "# create the environment\n",
    "env = TestEnv(\n",
    "    env_params=env_params,\n",
    "    sim_params=sim_params,\n",
    "    network=network\n",
    ")\n",
    "\n",
    "# run the simulation for 100000 steps\n",
    "exp = Experiment(env=env)\n",
    "#_ = exp.run(1, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the custom environment\n",
    "# Needs to be important in order to work properly in flow\n",
    "from flow.envs.simple_env import SimpleEnv\n",
    "env_name = SimpleEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=\"first_exp\",\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=Network,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=sim_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=VehicleParams(),\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments, run\n",
    "from ray.tune.experiment import Experiment\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "\n",
    "from ray.tune.schedulers import PopulationBasedTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 16:12:06,603\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-01_16-12-06_601887_13297/logs.\n",
      "2020-04-01 16:12:06,729\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:38580 to respond...\n",
      "2020-04-01 16:12:06,925\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:50392 to respond...\n",
      "2020-04-01 16:12:06,936\tINFO services.py:809 -- Starting Redis shard with 1.65 GB max memory.\n",
      "2020-04-01 16:12:07,064\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-04-01_16-12-06_601887_13297/logs.\n",
      "2020-04-01 16:12:07,071\tINFO services.py:1475 -- Starting the Plasma object store with 1.0 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.2.105',\n",
       " 'redis_address': '192.168.2.105:38580',\n",
       " 'object_store_address': '/tmp/ray/session_2020-04-01_16-12-06_601887_13297/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-04-01_16-12-06_601887_13297/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-04-01_16-12-06_601887_13297'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 1\n",
    "\n",
    "ray.init(num_cpus=N_CPUS, object_store_memory=1000000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(config):\n",
    "    # ensure we collect enough timesteps to do sgd\n",
    "    if config[\"train_batch_size\"] < config[\"sgd_minibatch_size\"] * 2:\n",
    "        config[\"train_batch_size\"] = config[\"sgd_minibatch_size\"] * 2\n",
    "    # ensure we run at least one sgd iter\n",
    "    if config[\"num_sgd_iter\"] < 1:\n",
    "        config[\"num_sgd_iter\"] = 1\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbt = PopulationBasedTraining(\n",
    "        time_attr=\"time_total_s\",\n",
    "        metric=\"episode_reward_mean\",\n",
    "        mode=\"max\",\n",
    "        perturbation_interval=4,\n",
    "        resample_probability=0.25,\n",
    "        # Specifies the mutations of these hyperparams\n",
    "        hyperparam_mutations={\n",
    "            \"lambda\": lambda: random.uniform(0.9, 1.0),\n",
    "            \"vf_clip_param\": lambda: random.uniform(20000, 50000),\n",
    "            \"lr\": [5e-2, 1e-3, 5e-4, 1e-4, 5e-5, 1e-5],\n",
    "            \"sgd_minibatch_size\": lambda: random.randint(128, 16384),\n",
    "            \"train_batch_size\": lambda: random.randint(N_CPUS*HORIZON, 2*N_CPUS*HORIZON),\n",
    "        },\n",
    "        custom_explore_fn=explore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "# #      \"class registered in the tune registry.\")\n",
    "alg_run = \"DQN\"\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = 0  # number of parallel workers\n",
    "# config[\"num_envs_per_worker\"] = 1  # number of parallel workers\n",
    "config[\"num_gpus\"] = 0\n",
    "config[\"lr\"] = 1e-3\n",
    "# config[\"v_max\"] = 0\n",
    "# config[\"v_min\"] = -50000\n",
    "config[\"train_batch_size\"] = 128  # batch size\n",
    "config[\"sample_batch_size\"] = 16  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [128]})  # size of hidden layers in network\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "config[\"timesteps_per_iteration\"] = HORIZON  \n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "config[\"env\"] = gym_name\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = Experiment(flow_params[\"exp_tag\"], **{\n",
    "        \"run\": alg_run,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 5,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 5,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 100,  # 222number of iterations to stop after\n",
    "        },\n",
    "        \"num_samples\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 16:12:14,181\tINFO trial_runner.py:176 -- Starting a new experiment.\n",
      "2020-04-01 16:12:14,209\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2020-04-01 16:12:14,214\tWARNING logger.py:227 -- Could not instantiate <class 'ray.tune.logger.TFLogger'> - skipping.\n",
      "2020-04-01 16:12:14,216\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.2 GB\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_SimpleEnv-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21,814\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21,815\tDEBUG worker_set.py:135 -- Creating TF session {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21.816813: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21.855133: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2899885000 Hz\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21.856693: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e679d0 executing computations on platform Host. Devices:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21.856961: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21,859\tDEBUG rollout_worker.py:721 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:21,861\tDEBUG catalog.py:333 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fbcaf078278>: Box(16,) -> (16,)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/distributional_q_model.py:85: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use keras.layers.Dense instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Please use `layer.__call__` method instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,116\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fbcac4efc18>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>, 'prev_actions': None, 'prev_rewards': None, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(16,), Discrete(4), [], Tensor(\"default_policy/seq_lens:0\", shape=(?,), dtype=int32)) -> Tensor(\"default_policy/q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 128), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,120\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fbcac4efb38>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(16,), Discrete(4), [], None) -> Tensor(\"default_policy/q_func_2/fc_net/fc_out/Tanh:0\", shape=(?, 128), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/decorator_utils.py:145: GraphKeys.VARIABLES (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use `tf.GraphKeys.GLOBAL_VARIABLES` instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/agents/dqn/dqn_policy.py:126: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Use `tf.random.categorical` instead.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,382\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'q_values': <tf.Tensor 'default_policy/q_values:0' shape=(?, 4) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,387\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fbcac3e0e80>: ({'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(16,), Discrete(4), [], None) -> Tensor(\"default_policy/q_func_3/fc_net/fc_out/Tanh:0\", shape=(?, 128), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,423\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fbcac393cf8>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 16) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(16,), Discrete(4), [], None) -> Tensor(\"default_policy/target_q_func_1/fc_net/fc_out/Tanh:0\", shape=(?, 128), dtype=float32), []\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,465\tDEBUG catalog.py:434 -- Created model <ray.rllib.models.tf.fcnet_v1.FullyConnectedNetwork object at 0x7fbcac32d748>: ({'obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 16) dtype=float32>, 'is_training': <tf.Tensor 'default_policy/PlaceholderWithDefault:0' shape=() dtype=bool>} of Box(16,), Discrete(4), [], None) -> Tensor(\"default_policy/q_func_4/fc_net/fc_out/Tanh:0\", shape=(?, 128), dtype=float32), []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:22,921\tDEBUG tf_policy.py:214 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'actions': <tf.Tensor 'default_policy/actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'weights': <tf.Tensor 'default_policy/weights:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,113\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,114\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,114\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,115\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,116\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,117\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,117\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,118\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,119\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/kernel:0' shape=(16, 128) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,120\tDEBUG simple_q_policy.py:60 -- Update target op <tf.Variable 'default_policy/target_q_func/fc_out/bias:0' shape=(128,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,121\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fbcaf078518>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,121\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fbcaf078278>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,121\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fbcaf078128>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,128\tDEBUG rollout_worker.py:436 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x7fbc847b02b0> (<SimpleEnv<SimpleEnv-v0>>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.DQNTFPolicy object at 0x7fbcaf078518>}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,132\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/ray/rllib/policy/tf_policy.py:570: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Prefer Variable.assign which has equivalent behavior in 2.X.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,177\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 0}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:23,177\tINFO rollout_worker.py:451 -- Generating sample batch of size 16\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Loading configuration... done.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,451\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,452\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,454\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,455\tINFO sampler.py:407 -- Filtered obs: np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,459\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'obs': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_action': np.ndarray((), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,460\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,565\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       { 'q_values': np.ndarray((1, 4), dtype=float32, min=0.0, max=0.0, mean=0.0)})}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,619\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'eps_id': np.ndarray((16,), dtype=int64, min=1616082248.0, max=1616082248.0, mean=1616082248.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.688),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_rewards': np.ndarray((16,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'q_values': np.ndarray((16, 4), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'rewards': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         't': np.ndarray((16,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:12:26,620\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.75),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'eps_id': np.ndarray((16,), dtype=int64, min=1616082248.0, max=1616082248.0, mean=1616082248.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.688),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_rewards': np.ndarray((16,), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'q_values': np.ndarray((16, 4), dtype=float32, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'rewards': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             't': np.ndarray((16,), dtype=int64, min=0.0, max=15.0, mean=7.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,951\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.633),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'batch_indexes': np.ndarray((128,), dtype=int64, min=0.0, max=1019.0, mean=527.594),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'new_obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.41),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.402),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'rewards': np.ndarray((128,), dtype=int64, min=-28.0, max=0.0, mean=-22.555),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'weights': np.ndarray((128,), dtype=float64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(16, 128) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:01,952\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(128,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:02,147\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0010000000474974513,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'max_q': 1.5891256,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_q': 0.39432365,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_td_error': 21.676716,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'min_q': -0.8904492,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       'td_error': np.ndarray((128,), dtype=float32, min=0.0, max=27.986, mean=21.677)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Loading configuration... done.\n",
      "Result for DQN_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_16-13-26\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: -46827.0\n",
      "  episode_reward_mean: -46827.0\n",
      "  episode_reward_min: -46827.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 1\n",
      "  experiment_id: c6130e264f8c4bbca020274295c2db49\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 11.607\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        max_q: -0.6764649152755737\n",
      "        mean_q: -37.93580627441406\n",
      "        mean_td_error: -3.186582326889038\n",
      "        min_q: -40.05873489379883\n",
      "        model: {}\n",
      "    max_exploration: 1.0\n",
      "    min_exploration: 1.0\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 7936\n",
      "    num_target_updates: 3\n",
      "    opt_peak_throughput: 11028.301\n",
      "    opt_samples: 128.0\n",
      "    replay_time_ms: 11.408\n",
      "    sample_time_ms: 722.372\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 42.551648351648346\n",
      "    ram_util_percent: 73.8076923076923\n",
      "  pid: 13373\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 27.485234805311578\n",
      "    mean_inference_ms: 0.9738942612891552\n",
      "    mean_processing_ms: 2.274504904148878\n",
      "  time_since_restore: 63.600101709365845\n",
      "  time_this_iter_s: 63.600101709365845\n",
      "  time_total_s: 63.600101709365845\n",
      "  timestamp: 1585750406\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 1\n",
      "  trial_id: c93a6352\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=13373], 63 s, 1 iter, 2000 ts, -4.68e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:13:26,787\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 2000}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,151\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.5)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,151\tINFO sampler.py:407 -- Filtered obs: np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.5)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,152\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'obs': np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_action': 0,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_reward': -24,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,152\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,153\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       { 'q_values': np.ndarray((1, 4), dtype=float32, min=-134.853, max=-130.923, mean=-132.48)})}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,171\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.562)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,171\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,231\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.125),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'eps_id': np.ndarray((16,), dtype=int64, min=154453049.0, max=154453049.0, mean=154453049.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.539),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.535),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.062),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_rewards': np.ndarray((16,), dtype=int64, min=-26.0, max=-23.0, mean=-24.562),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'q_values': np.ndarray((16, 4), dtype=float32, min=-135.395, max=-130.354, mean=-132.421),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'rewards': np.ndarray((16,), dtype=int64, min=-26.0, max=-23.0, mean=-24.625),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         't': np.ndarray((16,), dtype=int64, min=1888.0, max=1903.0, mean=1895.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,233\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.125),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'eps_id': np.ndarray((16,), dtype=int64, min=154453049.0, max=154453049.0, mean=154453049.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.539),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.535),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.062),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_rewards': np.ndarray((16,), dtype=int64, min=-26.0, max=-23.0, mean=-24.562),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'q_values': np.ndarray((16, 4), dtype=float32, min=-135.395, max=-130.354, mean=-132.421),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'rewards': np.ndarray((16,), dtype=int64, min=-26.0, max=-23.0, mean=-24.625),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             't': np.ndarray((16,), dtype=int64, min=1888.0, max=1903.0, mean=1895.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'unroll_id': np.ndarray((16,), dtype=int64, min=243.0, max=243.0, mean=243.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.656),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'batch_indexes': np.ndarray((128,), dtype=int64, min=72.0, max=3903.0, mean=2338.969),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'new_obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.518),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.517),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'rewards': np.ndarray((128,), dtype=int64, min=-28.0, max=-10.0, mean=-24.281),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'weights': np.ndarray((128,), dtype=float64, min=0.012, max=0.054, mean=0.019)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,247\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,248\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,248\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,248\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(16, 128) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,248\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(128,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,251\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0010000000474974513,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'max_q': -89.37317,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_q': -131.51895,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_td_error': -1.1866219,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'min_q': -135.17169,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       'td_error': np.ndarray((128,), dtype=float32, min=-11.079, max=3.383, mean=-1.187)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:02,260\tINFO rollout_worker.py:451 -- Generating sample batch of size 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Loading configuration... done.\n",
      "Result for DQN_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_16-14-06\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: -46534.0\n",
      "  episode_reward_mean: -46680.5\n",
      "  episode_reward_min: -46827.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 2\n",
      "  experiment_id: c6130e264f8c4bbca020274295c2db49\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 17.01\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        max_q: -85.00012969970703\n",
      "        mean_q: -136.07464599609375\n",
      "        mean_td_error: -6.990324020385742\n",
      "        min_q: -139.55943298339844\n",
      "        model: {}\n",
      "    max_exploration: 0.804\n",
      "    min_exploration: 0.804\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 23936\n",
      "    num_target_updates: 7\n",
      "    opt_peak_throughput: 7524.986\n",
      "    opt_samples: 128.0\n",
      "    replay_time_ms: 12.744\n",
      "    sample_time_ms: 496.903\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 47.34464285714286\n",
      "    ram_util_percent: 73.8714285714286\n",
      "  pid: 13373\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 24.606349722968886\n",
      "    mean_inference_ms: 0.9515113632550957\n",
      "    mean_processing_ms: 1.9712004484871248\n",
      "  time_since_restore: 102.99305987358093\n",
      "  time_this_iter_s: 39.39295816421509\n",
      "  time_total_s: 102.99305987358093\n",
      "  timestamp: 1585750446\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 2\n",
      "  trial_id: c93a6352\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=13373], 102 s, 2 iter, 4000 ts, -4.67e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:06,201\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 4000}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m Loading configuration... done.\n",
      "Result for DQN_SimpleEnv-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2020-04-01_16-14-45\n",
      "  done: false\n",
      "  episode_len_mean: 2000.0\n",
      "  episode_reward_max: -46534.0\n",
      "  episode_reward_mean: -46673.0\n",
      "  episode_reward_min: -46827.0\n",
      "  episodes_this_iter: 1\n",
      "  episodes_total: 3\n",
      "  experiment_id: c6130e264f8c4bbca020274295c2db49\n",
      "  hostname: valentin-Aspire-V3-372\n",
      "  info:\n",
      "    grad_time_ms: 15.997\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_lr: 0.0010000000474974513\n",
      "        max_q: -0.6478631496429443\n",
      "        mean_q: -213.40672302246094\n",
      "        mean_td_error: 0.8898339867591858\n",
      "        min_q: -221.95960998535156\n",
      "        model: {}\n",
      "    max_exploration: 0.608\n",
      "    min_exploration: 0.608\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 39936\n",
      "    num_target_updates: 11\n",
      "    opt_peak_throughput: 8001.307\n",
      "    opt_samples: 128.0\n",
      "    replay_time_ms: 15.163\n",
      "    sample_time_ms: 457.297\n",
      "    update_time_ms: 0.003\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.2.105\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 46.34285714285714\n",
      "    ram_util_percent: 73.82857142857142\n",
      "  pid: 13373\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 23.02507802265157\n",
      "    mean_inference_ms: 0.9386898995344953\n",
      "    mean_processing_ms: 1.7894228509834498\n",
      "  time_since_restore: 142.45339560508728\n",
      "  time_this_iter_s: 39.46033573150635\n",
      "  time_total_s: 142.45339560508728\n",
      "  timestamp: 1585750485\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 3\n",
      "  trial_id: c93a6352\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_SimpleEnv-v0_0:\tRUNNING, [1 CPUs, 0 GPUs], [pid=13373], 142 s, 3 iter, 6000 ts, -4.67e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:14:45,672\tDEBUG trainer.py:353 -- updated global vars: {'timestep': 6000}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,264\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'info': {},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'obs': np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.562),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_action': 2,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'prev_reward': -25,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,264\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,265\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       { 'q_values': np.ndarray((1, 4), dtype=float32, min=-288.17, max=-286.234, mean=-287.066)})}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,281\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.562)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,281\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': {}}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,281\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.562)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,281\tINFO sampler.py:407 -- Filtered obs: np.ndarray((16,), dtype=int64, min=0.0, max=8.0, mean=1.562)\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,572\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'agent0': { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.188),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'eps_id': np.ndarray((16,), dtype=int64, min=1615608442.0, max=1615608442.0, mean=1615608442.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.566),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.57),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.25),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'prev_rewards': np.ndarray((16,), dtype=int64, min=-27.0, max=-22.0, mean=-25.125),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'q_values': np.ndarray((16, 4), dtype=float32, min=-289.134, max=-285.001, mean=-287.135),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'rewards': np.ndarray((16,), dtype=int64, min=-27.0, max=-22.0, mean=-25.062),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         't': np.ndarray((16,), dtype=int64, min=896.0, max=911.0, mean=903.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'unroll_id': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                         'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,574\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'data': { 'actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.188),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'agent_index': np.ndarray((16,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'dones': np.ndarray((16,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'eps_id': np.ndarray((16,), dtype=int64, min=1615608442.0, max=1615608442.0, mean=1615608442.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'infos': np.ndarray((16,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'new_obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.566),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'obs': np.ndarray((16, 16), dtype=int64, min=0.0, max=8.0, mean=1.57),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_actions': np.ndarray((16,), dtype=int64, min=0.0, max=3.0, mean=1.25),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'prev_rewards': np.ndarray((16,), dtype=int64, min=-27.0, max=-22.0, mean=-25.125),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'q_values': np.ndarray((16, 4), dtype=float32, min=-289.134, max=-285.001, mean=-287.135),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'rewards': np.ndarray((16,), dtype=int64, min=-27.0, max=-22.0, mean=-25.062),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             't': np.ndarray((16,), dtype=int64, min=896.0, max=911.0, mean=903.5),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'unroll_id': np.ndarray((16,), dtype=int64, min=431.0, max=431.0, mean=431.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m             'weights': np.ndarray((16,), dtype=int64, min=1.0, max=1.0, mean=1.0)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO rollout_worker.py:575 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'count': 128,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'policy_batches': { 'default_policy': { 'data': { 'actions': np.ndarray((128,), dtype=int64, min=0.0, max=3.0, mean=1.391),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'batch_indexes': np.ndarray((128,), dtype=int64, min=93.0, max=6907.0, mean=4272.961),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'dones': np.ndarray((128,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'new_obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.469),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'obs': np.ndarray((128, 16), dtype=int64, min=0.0, max=8.0, mean=1.471),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'rewards': np.ndarray((128,), dtype=int64, min=-29.0, max=-5.0, mean=-23.508),\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                                     'weights': np.ndarray((128,), dtype=float64, min=0.01, max=0.042, mean=0.022)},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                           'type': 'SampleBatch'}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m   'type': 'MultiAgentBatch'}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/hidden_0/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/kernel:0' shape=(256, 4) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/action_value/dense/bias:0' shape=(4,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/kernel:0' shape=(128, 256) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense/bias:0' shape=(256,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/kernel:0' shape=(256, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/state_value/dense_1/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,589\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/kernel:0' shape=(16, 128) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,590\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/q_func/fc_out/bias:0' shape=(128,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,594\tINFO rollout_worker.py:597 -- Training output:\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m { 'default_policy': { 'learner_stats': { 'cur_lr': 0.0010000000474974513,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'max_q': -65.355064,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_q': -280.85062,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'mean_td_error': -8.522585,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'min_q': -288.9645,\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                                          'model': {}},\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m                       'td_error': np.ndarray((128,), dtype=float32, min=-21.572, max=18.017, mean=-8.523)}}\n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=13373)\u001b[0m 2020-04-01 16:15:02,603\tINFO rollout_worker.py:451 -- Generating sample batch of size 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 16:15:05,679\tERROR trial_runner.py:550 -- Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\", line 498, in _process_trial\n",
      "    result = self.trial_executor.fetch_result(trial)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\", line 342, in fetch_result\n",
      "    result = ray.get(trial_future[0])\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/worker.py\", line 2247, in get\n",
      "    raise value\n",
      "ray.exceptions.RayTaskError: \u001b[36mray_DQN:train()\u001b[39m (pid=13373, host=valentin-Aspire-V3-372)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py\", line 372, in train\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer.py\", line 358, in train\n",
      "    result = Trainable.train(self)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/tune/trainable.py\", line 171, in train\n",
      "    result = self._train()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/agents/trainer_template.py\", line 126, in _train\n",
      "    fetches = self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/optimizers/sync_replay_optimizer.py\", line 115, in step\n",
      "    batch = self.workers.local_worker().sample()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/rollout_worker.py\", line 453, in sample\n",
      "    batches = [self.input_reader.next()]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 56, in next\n",
      "    batches = [self.get_data()]\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 97, in get_data\n",
      "    item = next(self.rollout_provider)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/evaluation/sampler.py\", line 334, in _env_runner\n",
      "    base_env.send_actions(actions_to_send)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/env/base_env.py\", line 332, in send_actions\n",
      "    self.vector_env.vector_step(action_vector)\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/ray/rllib/env/vector_env.py\", line 110, in vector_step\n",
      "    obs, r, done, info = self.envs[i].step(actions[i])\n",
      "  File \"/home/valentin/flow/flow/envs/base.py\", line 369, in step\n",
      "    self.k.simulation.simulation_step()\n",
      "  File \"/home/valentin/flow/flow/core/kernel/simulation/traci.py\", line 56, in simulation_step\n",
      "    self.kernel_api.simulationStep()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traci/connection.py\", line 323, in simulationStep\n",
      "    result = self._sendExact()\n",
      "  File \"/usr/local/lib/python3.6/dist-packages/traci/connection.py\", line 99, in _sendExact\n",
      "    raise FatalTraCIError(\"connection closed by SUMO\")\n",
      "traci.exceptions.FatalTraCIError: connection closed by SUMO\n",
      "\n",
      "2020-04-01 16:15:05,699\tINFO trial_runner.py:587 -- Attempting to recover trial state from last checkpoint.\n",
      "2020-04-01 16:15:05,709\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 1/1 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.2 GB\n",
      "Result logdir: /home/valentin/ray_results/first_exp\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - DQN_SimpleEnv-v0_0:\tRUNNING, 1 failures: /home/valentin/ray_results/first_exp/DQN_SimpleEnv-v0_0_2020-04-01_16-12-14nh2e_956/error_2020-04-01_16-15-05.txt, [1 CPUs, 0 GPUs], [pid=13373], 142 s, 3 iter, 6000 ts, -4.67e+04 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=13497)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=13497)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=13497)\u001b[0m non-resource variables are not supported in the long term\n",
      "\u001b[2m\u001b[36m(pid=13497)\u001b[0m Loading configuration... done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-01 16:15:14,925\tERROR worker.py:1616 -- print_logs: Error 111 connecting to 192.168.2.105:38580. Connection refused.\n",
      "2020-04-01 16:15:14,926\tERROR worker.py:1716 -- listen_error_messages_raylet: Error 111 connecting to 192.168.2.105:38580. Connection refused.\n",
      "2020-04-01 16:15:14,926\tERROR import_thread.py:89 -- ImportThread: Error 111 connecting to 192.168.2.105:38580. Connection refused.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cc0e7eb6843e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(experiments, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mtrial_executor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_on_failed_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             return_trials=True)\n\u001b[0m\u001b[1;32m    325\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, stop, config, resources_per_trial, num_samples, local_dir, upload_dir, trial_name_creator, loggers, sync_to_cloud, sync_to_driver, checkpoint_freq, checkpoint_at_end, keep_checkpoints_num, checkpoint_score_attr, global_checkpoint_period, export_formats, max_failures, restore, search_alg, scheduler, with_server, server_port, verbose, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, return_trials, ray_auto_init, sync_function)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mlast_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlast_debug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mDEBUG_PRINT_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trials\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_ids, num_returns, timeout)\u001b[0m\n\u001b[1;32m   2370\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2371\u001b[0m             \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2372\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2373\u001b[0m         )\n\u001b[1;32m   2374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.RayletClient.wait\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/exceptions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, client_exc)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trials = run_experiments(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
